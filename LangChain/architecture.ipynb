{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b2784ac",
   "metadata": {},
   "source": [
    "# OSS (LangChain) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ddfdc",
   "metadata": {},
   "source": [
    "## Install and helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940d5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q langchain-core\n",
    "%pip install -q langchain-openai\n",
    "%pip install -q langchain-experimental\n",
    "%pip install --pre -U -q langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce091a",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "366e4a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, BaseMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "local_model_id = \"qwen2.5-coder:7b-instruct-q8_0\"\n",
    "\n",
    "# Pretty print\n",
    "def pretty_print(response:dict):\n",
    "    \"\"\"Prints the content of each message in the response dictionary.\"\"\"\n",
    "    messages = response[\"messages\"]\n",
    "    for message in messages:\n",
    "        print(message.content)\n",
    "\n",
    "def print_dictionary(response:dict):\n",
    "    # This method prints each message's content, role, and its metadata, with indentation for readability\n",
    "    messages = response.get(\"messages\", [])\n",
    "    for idx, message in enumerate(messages):\n",
    "        role = getattr(message, \"role\", message.__class__.__name__.replace(\"Message\", \"\").lower())\n",
    "        print(f\"Message {idx+1}:\")\n",
    "        print(f\"    Role: {role}\")\n",
    "        print(f\"    Content: {message.content}\\n\")\n",
    "        # Print additional metadata if present\n",
    "        if hasattr(message, \"response_metadata\") and message.response_metadata:\n",
    "            print(f\"    Metadata: {message.response_metadata}\\n\")\n",
    "        if hasattr(message, \"additional_kwargs\") and message.additional_kwargs:\n",
    "            print(f\"    Additional kwargs: {message.additional_kwargs}\\n\")\n",
    "        if hasattr(message, \"usage_metadata\") and getattr(message, \"usage_metadata\", None):\n",
    "            print(f\"    Usage metadata: {message.usage_metadata}\\n\")\n",
    "\n",
    "\n",
    "def create_initial_message(text=\"Hi!\"):\n",
    "    \"\"\"Creates a message instance with the given text.\"\"\"\n",
    "    \n",
    "    return {\"messages\": [format_message(text)]}\n",
    "\n",
    "\n",
    "def format_message(text=\"Hi! My name is Bob.\", MessageClass: type = HumanMessage):\n",
    "    \"\"\"Creates a single message instance.\"\"\"\n",
    "    return MessageClass(content=text)\n",
    "\n",
    "\n",
    "def create_model(model_id = \"gpt-4.1-nano\", local = False):\n",
    "    # uses openAPI_key \n",
    "    if local:\n",
    "        model = ChatOllama(\n",
    "            model=model_id,\n",
    "            temperature=0,\n",
    "            )\n",
    "    else:\n",
    "        model = ChatOpenAI(\n",
    "            model=model_id,\n",
    "            timeout=30\n",
    "            )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8680dd4",
   "metadata": {},
   "source": [
    "### Create sequential thread_id for checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d788b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install psycopg2-binary\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "DB_URI = \"postgresql://thomas:postgres@localhost:5432/postgres?sslmode=disable\"\n",
    "\n",
    "def get_next_thread_id():\n",
    "    \"\"\"\n",
    "    Ensures the sequence exists and returns the next thread_id.\n",
    "    \"\"\"\n",
    "    with psycopg2.connect(DB_URI) as conn:\n",
    "        with conn.cursor() as cur:\n",
    "            # Create the sequence if it doesn't exist\n",
    "            cur.execute(\"\"\"\n",
    "                DO $$\n",
    "                BEGIN\n",
    "                    IF NOT EXISTS (SELECT 1 FROM pg_class WHERE relkind='S' AND relname='thread_id_seq') THEN\n",
    "                        CREATE SEQUENCE thread_id_seq START 1;\n",
    "                    END IF;\n",
    "                END\n",
    "                $$;\n",
    "            \"\"\")\n",
    "\n",
    "            # Get the next value from the sequence\n",
    "            cur.execute(\"SELECT nextval('thread_id_seq')\")\n",
    "            new_thread_id = cur.fetchone()[0]\n",
    "\n",
    "    return new_thread_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ccb9ed",
   "metadata": {},
   "source": [
    "## Simple processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "280a9dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-5-nano\", model_provider=\"openai\")\n",
    "\n",
    "query = input()\n",
    "if query:\n",
    "  result = model.invoke(query)\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e861c0e",
   "metadata": {},
   "source": [
    "## Simple Agent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da878537",
   "metadata": {},
   "source": [
    "#### Basic agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d3a94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 1:\n",
      "    Role: human\n",
      "    Content: What is 2+2\n",
      "\n",
      "Message 2:\n",
      "    Role: ai\n",
      "    Content: 2 + 2 equals 4.\n",
      "\n",
      "    Metadata: {'token_usage': {'completion_tokens': 8, 'prompt_tokens': 27, 'total_tokens': 35, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CK1xCOevXfNYa23yEAwOSTsT1qGtq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}\n",
      "\n",
      "    Additional kwargs: {'refusal': None}\n",
      "\n",
      "    Usage metadata: {'input_tokens': 27, 'output_tokens': 8, 'total_tokens': 35, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "tools = [calculate, ]\n",
    "agent = create_agent(\n",
    "    model, \n",
    "    tools=tools,\n",
    "    prompt=SystemMessage(content=\"You are a research assistant. Cite your sources.\")\n",
    ")\n",
    "\n",
    "\n",
    "# query = create_initial_message(input())\n",
    "if query:\n",
    "    # result = agent.invoke(query)\n",
    "    print_dictionary(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f890a",
   "metadata": {},
   "source": [
    "#### Choose model dynamicly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2cf2d3e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "format_message() got an unexpected keyword argument 'first'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m agent = create_agent(select_model, tools=[])\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Suppose this is the current agent state\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m state = AgentState(messages=[\u001b[43mformat_message\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHi\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMessageClass\u001b[49m\u001b[43m=\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m, format_message(\u001b[33m\"\u001b[39m\u001b[33mHi how may I assist you?\u001b[39m\u001b[33m\"\u001b[39m, AIMessage, first=false) ])\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Get the model instance that would be used for this state\u001b[39;00m\n\u001b[32m     21\u001b[39m model_instance = select_model(state, runtime=\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: format_message() got an unexpected keyword argument 'first'"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langgraph.runtime import Runtime\n",
    "\n",
    "\n",
    "def select_model(state: AgentState, runtime: Runtime) -> ChatOpenAI:\n",
    "    \"\"\"Choose model based on conversation complexity.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    message_count = len(messages)\n",
    "    if message_count < 3:\n",
    "        return ChatOpenAI(model=\"gpt-4.1-nano\").bind_tools(tools)\n",
    "    else:\n",
    "        return ChatOpenAI(model=\"gpt-5-nano\").bind_tools(tools) # select a better model for longer conversations\n",
    "\n",
    "agent = create_agent(select_model, tools=[])\n",
    "\n",
    "# Suppose this is the current agent state\n",
    "state = AgentState(messages=[format_message(\"Hi\", MessageClass=HumanMessage, first=False), format_message(\"Hi how may I assist you?\", AIMessage, first=false) ])\n",
    "\n",
    "# Get the model instance that would be used for this state\n",
    "model_instance = select_model(state, runtime=None)\n",
    "print(model_instance.model_name)\n",
    "\n",
    "state = AgentState(messages=[\n",
    "    format_message(\"Hello\"),\n",
    "    format_message(\"Hi how may I assist you?\", AIMessage), \n",
    "    format_message(\"Lorem ipsum\")]\n",
    "    )\n",
    "\n",
    "model_instance = select_model(state, runtime=None)\n",
    "print(model_instance.model_name) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9052a199",
   "metadata": {},
   "source": [
    "#### Use local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cdb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"J'aime programmer.\" additional_kwargs={} response_metadata={'model': 'qwen2.5-coder:7b-instruct-q8_0', 'created_at': '2025-09-29T05:55:21.770835Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10860239333, 'load_duration': 5122468292, 'prompt_eval_count': 33, 'prompt_eval_duration': 5244246084, 'eval_count': 6, 'eval_duration': 463770666, 'model_name': 'qwen2.5-coder:7b-instruct-q8_0'} id='run--eac17565-5cad-4167-b560-7beb7d497601-0' usage_metadata={'input_tokens': 33, 'output_tokens': 6, 'total_tokens': 39}\n"
     ]
    }
   ],
   "source": [
    "# %pip install -qU langchain-ollama\n",
    "from langchain_ollama import ChatOllama\n",
    "local_model_id = \"qwen2.5-coder:7b-instruct-q8_0\"\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=local_model_id,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "# ai_msg = llm.invoke(messages)\n",
    "# # pretty_print(ai_msg)\n",
    "# print(ai_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7729df95",
   "metadata": {},
   "source": [
    "#### Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b2c01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Ich liebe Programmieren.', additional_kwargs={}, response_metadata={'model': 'qwen2.5-coder:7b-instruct-q8_0', 'created_at': '2025-09-29T05:44:24.696387Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1417916167, 'load_duration': 62493333, 'prompt_eval_count': 28, 'prompt_eval_duration': 892272084, 'eval_count': 6, 'eval_duration': 455109541, 'model_name': 'qwen2.5-coder:7b-instruct-q8_0'}, id='run--12083a26-163f-4555-bf82-bdb1fc02726d-0', usage_metadata={'input_tokens': 28, 'output_tokens': 6, 'total_tokens': 34})"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"German\",\n",
    "        \"input\": \"I love programming.\",\n",
    "    }\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad66f237",
   "metadata": {},
   "source": [
    "#### Streaming the model invokes - for long queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(create_model(),[])\n",
    "\n",
    "for chunk in agent.stream({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}]\n",
    "}, stream_mode=\"values\"):\n",
    "    # Each chunk contains the full state at that point\n",
    "    latest_message = chunk[\"messages\"][-1]\n",
    "    if latest_message.content:\n",
    "        print(f\"Agent: {latest_message.content}\")\n",
    "    elif latest_message.tool_calls:\n",
    "        print(f\"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcae014",
   "metadata": {},
   "source": [
    "#### Set prompt Dynamicly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53227bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_prompt(state):\n",
    "    user_type = state.get(\"user_type\", \"standard\")\n",
    "    system_msg = SystemMessage(\n",
    "        content=\"Provide detailed technical responses.\" if user_type == \"expert\" else \"Provide simple, clear explanations.\"\n",
    "    )\n",
    "    return [system_msg] + state[\"messages\"]\n",
    "agent = create_agent(model, tools, prompt=dynamic_prompt)\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86202fd9",
   "metadata": {},
   "source": [
    "#### Multi-turn conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b879fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import filter_messages\n",
    "\n",
    "def manage_conversation_window(messages, max_messages=10):\n",
    "    \"\"\"Keep only the system message and last N messages\"\"\"\n",
    "    system_msgs = filter_messages(messages, include_types=\"system\")\n",
    "    recent_msgs = messages[-(max_messages-len(system_msgs)):]\n",
    "    return system_msgs + recent_msgs\n",
    "\n",
    "messages = [SystemMessage(content=\"Your a helpful assistant\")]\n",
    "\n",
    "# Usage in conversation loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    messages.append(HumanMessage(content=user_input))\n",
    "\n",
    "    # Trim conversation to fit context window\n",
    "    messages = manage_conversation_window(messages)\n",
    "\n",
    "    response = model.invoke(messages)\n",
    "    messages.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dab5a2",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82fd7cf",
   "metadata": {},
   "source": [
    "### Create basic tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8762da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for information.\"\"\"\n",
    "    return f\"Results for: {query}\"\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Perform calculations.\"\"\"\n",
    "    return str(5)\n",
    "    return str(eval(expression))\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_database(query: str, limit: int = 10) -> str:\n",
    "    \"\"\"Search the customer database for records matching the query.\n",
    "\n",
    "    Args:\n",
    "        query: Search terms to look for\n",
    "        limit: Maximum number of results to return\n",
    "    \"\"\"\n",
    "    return f\"Found {limit} results for '{query}'\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7666c",
   "metadata": {},
   "source": [
    "### Create structured output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebdd860",
   "metadata": {},
   "source": [
    "#### Simple output format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2583446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\", additional_kwargs={}, response_metadata={}, id='d37a43b1-cd82-4cc5-a1fb-5c7ec3aa05c0'),\n",
       "  AIMessage(content='{\\n  \"name\": \"ProductReview\",\\n  \"arguments\": {\\n    \"rating\": 5,\\n    \"sentiment\": \"positive\",\\n    \"key_points\": [\\n      \"great product\",\\n      \"fast shipping\",\\n      \"expensive\"\\n    ]\\n  }\\n}', additional_kwargs={}, response_metadata={'model': 'qwen2.5-coder:7b-instruct-q8_0', 'created_at': '2025-09-29T06:20:48.203524Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7089659708, 'load_duration': 63446958, 'prompt_eval_count': 537, 'prompt_eval_duration': 1636961500, 'eval_count': 56, 'eval_duration': 5353350166, 'model_name': 'qwen2.5-coder:7b-instruct-q8_0'}, id='run--ac7fd276-b2d7-4c73-936a-684fbbf3e946-0', usage_metadata={'input_tokens': 537, 'output_tokens': 56, 'total_tokens': 593})]}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "class ProductReview(BaseModel):\n",
    "    \"\"\"Analysis of a product review.\"\"\"\n",
    "    rating: Optional[int] = Field(description=\"The rating of the product\", ge=1, le=5)\n",
    "    sentiment: Literal[\"positive\", \"negative\"] = Field(description=\"The sentiment of the review\")\n",
    "    key_points: list[str] = Field(description=\"The key points of the review. Lowercase, 1-3 words each.\")\n",
    "\n",
    "model = create_model(local_model_id, True)\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=tools,\n",
    "    response_format=ProductReview\n",
    "\n",
    ")\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}]\n",
    "})\n",
    "# result[\"structured_response\"]\n",
    "result[\"messages\"][1]\n",
    "result\n",
    "# ProductReview(rating=5, sentiment='positive', key_points=['fast shipping', 'expensive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8947f56",
   "metadata": {},
   "source": [
    "#### Add custom message tool message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127cfb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='{\\n  \"name\": \"MeetingAction\",\\n  \"arguments\": {\\n    \"task\": \"update the project timeline\",\\n    \"assignee\": \"Sarah\",\\n    \"priority\": \"high\"\\n  }\\n}', additional_kwargs={}, response_metadata={'model': 'qwen2.5-coder:7b-instruct-q8_0', 'created_at': '2025-09-29T06:03:18.942758Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5015568167, 'load_duration': 59698667, 'prompt_eval_count': 225, 'prompt_eval_duration': 1138130416, 'eval_count': 43, 'eval_duration': 3811779625, 'model_name': 'qwen2.5-coder:7b-instruct-q8_0'}, id='run--ad5599d5-6232-462b-818d-dcd975cf4dc5-0', usage_metadata={'input_tokens': 225, 'output_tokens': 43, 'total_tokens': 268})"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "class MeetingAction(BaseModel):\n",
    "    \"\"\"Action items extracted from a meeting transcript.\"\"\"\n",
    "    task: str = Field(description=\"The specific task to be completed\")\n",
    "    assignee: str = Field(description=\"Person responsible for the task\")\n",
    "    priority: Literal[\"low\", \"medium\", \"high\"] = Field(description=\"Priority level\")\n",
    "model = create_model(local_model_id, True)\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    response_format=ToolStrategy(\n",
    "        schema=MeetingAction,\n",
    "        tool_message_content=\"Action item captured and added to meeting notes!\"\n",
    "    )\n",
    ")\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"From our meeting: Sarah needs to update the project timeline as soon as possible\"}]\n",
    "})\n",
    "\n",
    "result[\"messages\"][1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a8a969",
   "metadata": {},
   "source": [
    "#### Structured Output Error Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6698e1e3",
   "metadata": {},
   "source": [
    "##### Schema validations errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c136ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"name\": \"ProductRating\", \"arguments\": {\"rating\": 5, \"comment\": \"Amazing product, 10/10!\"}}'"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "class ProductRating(BaseModel):\n",
    "    rating: Optional[int] = Field(description=\"Rating from 1-5\", ge=1, le=5)\n",
    "    comment: str = Field(description=\"Review comment\")\n",
    "    # uml: list[str] = Field\n",
    "\n",
    "model = create_model(local_model_id,True)\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    response_format=ToolStrategy(ProductRating),  # Default: handle_errors=True\n",
    "    prompt=\"You are a helpful assistant that parses product reviews. Do not make any field or value up.\"\n",
    ")\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Parse this: Amazing product, 10/10!\"}]\n",
    "})\n",
    "result[\"messages\"][1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa4230",
   "metadata": {},
   "source": [
    "##### Define errors to retry the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2ba9104f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolStrategy(schema=<class '__main__.ProductRating'>, schema_specs=[_SchemaSpec(schema=<class '__main__.ProductRating'>, name='ProductRating', description='', schema_kind='pydantic', json_schema={'properties': {'rating': {'anyOf': [{'maximum': 5, 'minimum': 1, 'type': 'integer'}, {'type': 'null'}], 'description': 'Rating from 1-5', 'title': 'Rating'}, 'comment': {'description': 'Review comment', 'title': 'Comment', 'type': 'string'}}, 'required': ['rating', 'comment'], 'title': 'ProductRating', 'type': 'object'}, strict=False)], tool_message_content=None, handle_errors=(<class 'ValueError'>, <class 'TypeError'>))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ToolStrategy(\n",
    "    schema=ProductRating,\n",
    "    handle_errors=(ValueError, TypeError)  # Retry on ValueError and TypeError otherwise raise the exception\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1e0597",
   "metadata": {},
   "source": [
    "##### Custom error handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "963f9473",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ToolStrategy' object has no attribute '__mro__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/pydantic/type_adapter.py:270\u001b[39m, in \u001b[36mTypeAdapter._init_core_attrs\u001b[39m\u001b[34m(self, ns_resolver, force, raise_errors)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m     \u001b[38;5;28mself\u001b[39m.core_schema = \u001b[43m_getattr_no_parents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m__pydantic_core_schema__\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28mself\u001b[39m.validator = _getattr_no_parents(\u001b[38;5;28mself\u001b[39m._type, \u001b[33m'\u001b[39m\u001b[33m__pydantic_validator__\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/pydantic/type_adapter.py:55\u001b[39m, in \u001b[36m_getattr_no_parents\u001b[39m\u001b[34m(obj, attribute)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attribute)\n",
      "\u001b[31mAttributeError\u001b[39m: __pydantic_core_schema__",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[148]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     15\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(error)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mToolStrategy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mToolStrategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUnion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mProductReview\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEventDetails\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandle_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_error_handler\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langchain/agents/structured_output.py:232\u001b[39m, in \u001b[36mToolStrategy.__init__\u001b[39m\u001b[34m(self, schema, tool_message_content, handle_errors)\u001b[39m\n\u001b[32m    228\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m schema\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28mself\u001b[39m.schema_specs = [\u001b[43m_SchemaSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m _iter_variants(schema)]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langchain/agents/structured_output.py:160\u001b[39m, in \u001b[36m_SchemaSpec.__init__\u001b[39m\u001b[34m(self, schema, name, description, strict)\u001b[39m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_dataclass(schema):\n\u001b[32m    159\u001b[39m     \u001b[38;5;28mself\u001b[39m.schema_kind = \u001b[33m\"\u001b[39m\u001b[33mdataclass\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28mself\u001b[39m.json_schema = \u001b[43mTypeAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m.json_schema()\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_typeddict(schema):\n\u001b[32m    162\u001b[39m     \u001b[38;5;28mself\u001b[39m.schema_kind = \u001b[33m\"\u001b[39m\u001b[33mtypeddict\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/pydantic/type_adapter.py:227\u001b[39m, in \u001b[36mTypeAdapter.__init__\u001b[39m\u001b[34m(self, type, config, _parent_depth, module)\u001b[39m\n\u001b[32m    224\u001b[39m     localns = {}\n\u001b[32m    226\u001b[39m \u001b[38;5;28mself\u001b[39m._module_name = module \u001b[38;5;129;01mor\u001b[39;00m cast(\u001b[38;5;28mstr\u001b[39m, globalns.get(\u001b[33m'\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_core_attrs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mns_resolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_namespace_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNsResolver\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnamespaces_tuple\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_namespace_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNamespacesTuple\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mlocalns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mglobalns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparent_namespace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocalns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/pydantic/type_adapter.py:289\u001b[39m, in \u001b[36mTypeAdapter._init_core_attrs\u001b[39m\u001b[34m(self, ns_resolver, force, raise_errors)\u001b[39m\n\u001b[32m    286\u001b[39m schema_generator = _generate_schema.GenerateSchema(config_wrapper, ns_resolver=ns_resolver)\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     core_schema = \u001b[43mschema_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PydanticUndefinedAnnotation:\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raise_errors:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:711\u001b[39m, in \u001b[36mGenerateSchema.generate_schema\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m    708\u001b[39m schema = \u001b[38;5;28mself\u001b[39m._generate_schema_from_get_schema_method(obj, obj)\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m711\u001b[39m     schema = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_schema_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    713\u001b[39m metadata_js_function = _extract_get_pydantic_json_schema(obj)\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadata_js_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1009\u001b[39m, in \u001b[36mGenerateSchema._generate_schema_inner\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, PydanticRecursiveRef):\n\u001b[32m   1007\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m core_schema.definition_reference_schema(schema_ref=obj.type_ref)\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmatch_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1119\u001b[39m, in \u001b[36mGenerateSchema.match_type\u001b[39m\u001b[34m(self, obj)\u001b[39m\n\u001b[32m   1116\u001b[39m \u001b[38;5;66;03m# dataclasses.is_dataclass coerces dc instances to types, but we only handle\u001b[39;00m\n\u001b[32m   1117\u001b[39m \u001b[38;5;66;03m# the case of a dc type here\u001b[39;00m\n\u001b[32m   1118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dataclasses.is_dataclass(obj):\n\u001b[32m-> \u001b[39m\u001b[32m1119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataclass_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pyright: ignore[reportArgumentType]\u001b[39;00m\n\u001b[32m   1121\u001b[39m origin = get_origin(obj)\n\u001b[32m   1122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/pydantic/_internal/_generate_schema.py:1942\u001b[39m, in \u001b[36mGenerateSchema._dataclass_schema\u001b[39m\u001b[34m(self, dataclass, origin)\u001b[39m\n\u001b[32m   1940\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m PydanticUndefinedAnnotation.from_name_error(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1941\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1942\u001b[39m     fields = \u001b[43mcollect_dataclass_fields\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataclass\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtypevars_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypevars_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_config_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1948\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._config_wrapper.extra == \u001b[33m'\u001b[39m\u001b[33mallow\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m   1949\u001b[39m     \u001b[38;5;66;03m# disallow combination of init=False on a dataclass field and extra='allow' on a dataclass\u001b[39;00m\n\u001b[32m   1950\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m field_name, field \u001b[38;5;129;01min\u001b[39;00m fields.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/pydantic/_internal/_fields.py:372\u001b[39m, in \u001b[36mcollect_dataclass_fields\u001b[39m\u001b[34m(cls, ns_resolver, typevars_map, config_wrapper)\u001b[39m\n\u001b[32m    367\u001b[39m dataclass_fields = \u001b[38;5;28mcls\u001b[39m.__dataclass_fields__\n\u001b[32m    369\u001b[39m \u001b[38;5;66;03m# The logic here is similar to `_typing_extra.get_cls_type_hints`,\u001b[39;00m\n\u001b[32m    370\u001b[39m \u001b[38;5;66;03m# although we do it manually as stdlib dataclasses already have annotations\u001b[39;00m\n\u001b[32m    371\u001b[39m \u001b[38;5;66;03m# collected in each class:\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m base \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__mro__\u001b[39;49m):\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dataclasses.is_dataclass(base):\n\u001b[32m    374\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'ToolStrategy' object has no attribute '__mro__'"
     ]
    }
   ],
   "source": [
    "from typing import Union\n",
    "from langchain.agents.structured_output import MultipleStructuredOutputsError, StructuredOutputValidationError \n",
    "\n",
    "class EventDetails(BaseModel):\n",
    "    location: str = Field(description=\"Location of the event\")\n",
    "    description: str = Field(description=\"Description of the event\")\n",
    "\n",
    "\n",
    "def custom_error_handler(error: Exception) -> str:\n",
    "    if isinstance(error, StructuredOutputValidationError):\n",
    "        return \"There was an issue with the format. Try again.\"\n",
    "    elif isinstance(error, MultipleStructuredOutputsError):\n",
    "        return \"Multiple structured outputs were returned. Pick the most relevant one.\"\n",
    "    else:\n",
    "        return f\"Error: {str(error)}\"\n",
    "\n",
    "ToolStrategy(\n",
    "    schema=ToolStrategy(Union[ProductReview, EventDetails]),\n",
    "    handle_errors=custom_error_handler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad7fa6",
   "metadata": {},
   "source": [
    "### ToolNode with error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5e6ad0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"search\",\n",
      "  \"arguments\": {\n",
      "    \"query\": \"rime\"\n",
      "  }\n",
      "}\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import ToolNode\n",
    "\n",
    "tool_node = ToolNode(\n",
    "    tools=[search, calculate],\n",
    "    handle_tool_errors=\"Please check your input and try again.\"\n",
    ")\n",
    "agent = create_agent(model, tools=tool_node)\n",
    "\n",
    "query = create_initial_message(input())\n",
    "if query:\n",
    "    result = agent.invoke(query)\n",
    "\n",
    "    print(result[\"messages\"][1].content)\n",
    "    print(result[\"messages\"][1].tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb8c52",
   "metadata": {},
   "source": [
    "## Setup DB, RAG and vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b6888",
   "metadata": {},
   "source": [
    "### RAG from website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5834e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q markdownify\n",
    "import requests\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from markdownify import markdownify\n",
    "\n",
    "ALLOWED_DOMAINS = [\"https://langchain-ai.github.io/\"]\n",
    "LLMS_TXT = 'https://langchain-ai.github.io/langgraph/llms.txt'\n",
    "\n",
    "\n",
    "@tool\n",
    "def fetch_documentation(url: str) -> str:  \n",
    "    \"\"\"Fetch and convert documentation from a URL\"\"\"\n",
    "    if not any(url.startswith(domain) for domain in ALLOWED_DOMAINS):\n",
    "        return (\n",
    "            \"Error: URL not allowed. \"\n",
    "            f\"Must start with one of: {', '.join(ALLOWED_DOMAINS)}\"\n",
    "        )\n",
    "    response = requests.get(url, timeout=10.0)\n",
    "    response.raise_for_status()\n",
    "    return markdownify(response.text)\n",
    "\n",
    "\n",
    "# We will fetch the content of llms.txt, so this can\n",
    "# be done ahead of time without requiring an LLM request.\n",
    "llms_txt_content = requests.get(LLMS_TXT).text\n",
    "\n",
    "# System prompt for the agent\n",
    "system_prompt = f\"\"\"\n",
    "You are an expert Python developer and technical assistant.\n",
    "Your primary role is to help users with questions about LangGraph and related tools.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "1. If a user asks a question you're unsure about — or one that likely involves API usage,\n",
    "   behavior, or configuration — you MUST use the `fetch_documentation` tool to consult the relevant docs.\n",
    "2. When citing documentation, summarize clearly and include relevant context from the content.\n",
    "3. Do not use any URLs outside of the allowed domain.\n",
    "4. If a documentation fetch fails, tell the user and proceed with your best expert understanding.\n",
    "\n",
    "You can access official documentation from the following approved sources:\n",
    "\n",
    "{llms_txt_content}\n",
    "\n",
    "You MUST consult the documentation to get up to date documentation\n",
    "before answering a user's question about LangGraph.\n",
    "\n",
    "Your answers should be clear, concise, and technically accurate.\n",
    "\"\"\"\n",
    "\n",
    "tools = [fetch_documentation]\n",
    "\n",
    "model = init_chat_model(\"claude-sonnet-4-0\", max_tokens=32_000)\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=tools,  \n",
    "    prompt=system_prompt,  \n",
    "    name=\"Agentic RAG\",\n",
    ")\n",
    "\n",
    "response = agent.invoke({\n",
    "    'messages': [{\n",
    "        'role': 'user',\n",
    "        'content': (\n",
    "            \"Write a short example of a langgraph agent using the \"\n",
    "            \"prebuilt create react agent. the agent should be able \"\n",
    "            \"to look up stock pricing information.\"\n",
    "        )\n",
    "    }]\n",
    "})\n",
    "\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54eb007",
   "metadata": {},
   "source": [
    "### Setup chromaDB for local RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfa94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q chromadb\n",
    "\n",
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# switch `create_collection` to `get_or_create_collection` to avoid creating a new collection every time\n",
    "collection = chroma_client.get_or_create_collection(name=\"my_collection\")\n",
    "\n",
    "# switch `add` to `upsert` to avoid adding the same documents every time\n",
    "collection.upsert(\n",
    "    documents=[\n",
    "        \"This is a document about pineapple\",\n",
    "        \"This is a document about oranges\"\n",
    "    ],\n",
    "    ids=[\"id1\", \"id2\"]\n",
    ")\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[\"This is a query document about florida\"], # Chroma will embed this for you\n",
    "    n_results=2 # how many results to return\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e6e49e",
   "metadata": {},
   "source": [
    "### RAG - Local source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd77e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain-chroma\n",
    "\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "raw_documents = TextLoader('state_of_the_union.txt').load()\n",
    "\n",
    "query = input()\n",
    "\n",
    "response = model.invoke(query)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection_name = \"user\"\n",
    "chroma_client.get_or_create_collection(name=collection_name)\n",
    "chroma_client.a\n",
    "\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd2f131",
   "metadata": {},
   "source": [
    "### Create In memory vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "vector_store.add_texts(\"This is a cool text\")\n",
    "vector_store.add_documents()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8a0b3",
   "metadata": {},
   "source": [
    "## Human in the loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458e8d1",
   "metadata": {},
   "source": [
    "### Setup middleware and tool configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aef974bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Function must have a docstring if description not provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     15\u001b[39m tool_cfg = tool_configs={\n\u001b[32m     16\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mwrite_file\u001b[39m\u001b[33m\"\u001b[39m: {\n\u001b[32m     17\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mallow_accept\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mread_data\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Safe operation, no approval needed\u001b[39;00m\n\u001b[32m     27\u001b[39m             }\n\u001b[32m     29\u001b[39m hitl = HumanInTheLoopMiddleware(tool_cfg, description_prefix=\u001b[33m\"\u001b[39m\u001b[33mTool execution pending approval\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m agent = \u001b[43mcreate_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mwrite_file_tool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecute_sql_tool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_data_tool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmiddleware\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhitl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mInMemorySaver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Required for interrupts\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langchain/agents/react_agent.py:1151\u001b[39m, in \u001b[36mcreate_agent\u001b[39m\u001b[34m(model, tools, middleware, prompt, response_format, pre_model_hook, post_model_hook, state_schema, context_schema, checkpointer, store, interrupt_before, interrupt_after, debug, version, name, **deprecated_kwargs)\u001b[39m\n\u001b[32m   1149\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m post_model_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# noqa: S101\u001b[39;00m\n\u001b[32m   1150\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m state_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# noqa: S101\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_middleware_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[32m   1152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m        \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmiddleware\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmiddleware\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontext_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.compile(\n\u001b[32m   1159\u001b[39m         checkpointer=checkpointer,\n\u001b[32m   1160\u001b[39m         store=store,\n\u001b[32m   1161\u001b[39m         name=name,\n\u001b[32m   1162\u001b[39m         interrupt_after=interrupt_after,\n\u001b[32m   1163\u001b[39m         interrupt_before=interrupt_before,\n\u001b[32m   1164\u001b[39m         debug=debug,\n\u001b[32m   1165\u001b[39m     )\n\u001b[32m   1167\u001b[39m \u001b[38;5;66;03m# Handle deprecated config_schema parameter\u001b[39;00m\n\u001b[32m   1168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (config_schema := deprecated_kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mconfig_schema\u001b[39m\u001b[33m\"\u001b[39m, MISSING)) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m MISSING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langchain/agents/middleware_agent.py:192\u001b[39m, in \u001b[36mcreate_agent\u001b[39m\u001b[34m(model, tools, system_prompt, middleware, response_format, context_schema)\u001b[39m\n\u001b[32m    189\u001b[39m     all_tools = middleware_tools + regular_tools + structured_tools\n\u001b[32m    191\u001b[39m     \u001b[38;5;66;03m# Only create ToolNode if we have tools\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     tool_node = \u001b[43mToolNode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_tools\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m all_tools \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    193\u001b[39m     default_tools = regular_tools + builtin_tools + structured_tools + middleware_tools\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tools, ToolNode):\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# tools is ToolNode or None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langchain/agents/tool_node.py:415\u001b[39m, in \u001b[36mToolNode.__init__\u001b[39m\u001b[34m(self, tools, name, tags, handle_tool_errors, messages_key)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools:\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool, BaseTool):\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m         tool_ = \u001b[43mcreate_tool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype[BaseTool]\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    417\u001b[39m         tool_ = tool\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langchain_core/tools/convert.py:332\u001b[39m, in \u001b[36mtool\u001b[39m\u001b[34m(name_or_callable, runnable, description, return_direct, args_schema, infer_schema, response_format, parse_docstring, error_on_invalid_docstring, *args)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name_or_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    327\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(name_or_callable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(name_or_callable, \u001b[33m\"\u001b[39m\u001b[33m__name__\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    328\u001b[39m         \u001b[38;5;66;03m# Used as a decorator without parameters\u001b[39;00m\n\u001b[32m    329\u001b[39m         \u001b[38;5;66;03m# @tool\u001b[39;00m\n\u001b[32m    330\u001b[39m         \u001b[38;5;66;03m# def my_tool():\u001b[39;00m\n\u001b[32m    331\u001b[39m         \u001b[38;5;66;03m#    pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_tool_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_callable\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_callable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name_or_callable, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    334\u001b[39m         \u001b[38;5;66;03m# Used with a new name for the tool\u001b[39;00m\n\u001b[32m    335\u001b[39m         \u001b[38;5;66;03m# @tool(\"search\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    342\u001b[39m         \u001b[38;5;66;03m# def my_tool():\u001b[39;00m\n\u001b[32m    343\u001b[39m         \u001b[38;5;66;03m#    pass\u001b[39;00m\n\u001b[32m    344\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _create_tool_factory(name_or_callable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langchain_core/tools/convert.py:277\u001b[39m, in \u001b[36mtool.<locals>._create_tool_factory.<locals>._tool_factory\u001b[39m\u001b[34m(dec_func)\u001b[39m\n\u001b[32m    274\u001b[39m     schema = args_schema\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m infer_schema \u001b[38;5;129;01mor\u001b[39;00m args_schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStructuredTool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoroutine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtool_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_direct\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_direct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43minfer_schema\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_schema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparse_docstring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparse_docstring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_on_invalid_docstring\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_on_invalid_docstring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[38;5;66;03m# If someone doesn't want a schema applied, we must treat it as\u001b[39;00m\n\u001b[32m    290\u001b[39m \u001b[38;5;66;03m# a simple string->string function\u001b[39;00m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dec_func.\u001b[34m__doc__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langchain_core/tools/structured.py:242\u001b[39m, in \u001b[36mStructuredTool.from_function\u001b[39m\u001b[34m(cls, func, coroutine, name, description, return_direct, args_schema, infer_schema, response_format, parse_docstring, error_on_invalid_docstring, **kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m description_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    241\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mFunction must have a docstring if description not provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m description \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    244\u001b[39m     \u001b[38;5;66;03m# Only apply if using the function's docstring\u001b[39;00m\n\u001b[32m    245\u001b[39m     description_ = textwrap.dedent(description_).strip()\n",
      "\u001b[31mValueError\u001b[39m: Function must have a docstring if description not provided."
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "def write_file_tool():\n",
    "    return \"succes\"\n",
    "\n",
    "def execute_sql_tool():\n",
    "    return \"succes\"\n",
    "\n",
    "def read_data_tool():\n",
    "    return \"succes\"\n",
    "\n",
    "\n",
    "tool_cfg = tool_configs={\n",
    "                \"write_file\": {\n",
    "                    \"allow_accept\": True,\n",
    "                    \"allow_edit\": True,\n",
    "                    \"allow_respond\": True,\n",
    "                    \"description\": \"⚠️ File write operation requires approval\",\n",
    "                },\n",
    "                \"execute_sql\": {\n",
    "                    \"allow_accept\": True,\n",
    "                    \"description\": \"🚨 SQL execution requires careful review\",\n",
    "                },\n",
    "                \"read_data\": False,  # Safe operation, no approval needed\n",
    "            }\n",
    "\n",
    "hitl = HumanInTheLoopMiddleware(tool_cfg, description_prefix=\"Tool execution pending approval\")\n",
    "\n",
    "agent = create_agent(\n",
    "    model=create_model(),\n",
    "    tools=[write_file_tool, execute_sql_tool, read_data_tool],\n",
    "    middleware=[hitl],\n",
    "    checkpointer=InMemorySaver(),  # Required for interrupts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea7d01",
   "metadata": {},
   "source": [
    "#### Test hitl middleware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2cc67568",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "the connection is closed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Initial invocation\u001b[39;00m\n\u001b[32m      6\u001b[39m config = RunnableConfig({\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mHuman_in_the_loop\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m})\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDelete old records from the database\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Check if paused for approval\u001b[39;00m\n\u001b[32m     15\u001b[39m state = agent.get_state(config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3024\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3026\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3031\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3032\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3034\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3035\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3036\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3038\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3039\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3040\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2582\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2579\u001b[39m runtime = parent_runtime.merge(runtime)\n\u001b[32m   2580\u001b[39m config[CONF][CONFIG_KEY_RUNTIME] = runtime\n\u001b[32m-> \u001b[39m\u001b[32m2582\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSyncPregelLoop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2583\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2584\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStreamProtocol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_modes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2585\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2586\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2587\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2588\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2589\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2590\u001b[39m \u001b[43m    \u001b[49m\u001b[43mspecs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2591\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2592\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_channels_asis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2594\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2595\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2596\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2598\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrigger_to_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrigger_to_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2599\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmigrate_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_migrate_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2600\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2601\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcache_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2602\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2603\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# create runner\u001b[39;49;00m\n\u001b[32m   2604\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mPregelRunner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2605\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCONF\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2606\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_RUNNER_SUBMIT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mWeakMethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2609\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnode_finished\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[43mCONF\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG_KEY_NODE_FINISHED\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2610\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2611\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# enable subgraph streaming\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langgraph/pregel/_loop.py:1007\u001b[39m, in \u001b[36mSyncPregelLoop.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Self:\n\u001b[32m   1006\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.checkpointer:\n\u001b[32m-> \u001b[39m\u001b[32m1007\u001b[39m         saved = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpointer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpoint_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1008\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1009\u001b[39m         saved = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langgraph/checkpoint/postgres/__init__.py:226\u001b[39m, in \u001b[36mPostgresSaver.get_tuple\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    223\u001b[39m     args = (thread_id, checkpoint_ns)\n\u001b[32m    224\u001b[39m     where = \u001b[33m\"\u001b[39m\u001b[33mWHERE thread_id = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m AND checkpoint_ns = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m ORDER BY checkpoint_id DESC LIMIT 1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cursor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcur\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mSELECT_SQL\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetchone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/contextlib.py:141\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langgraph/checkpoint/postgres/__init__.py:430\u001b[39m, in \u001b[36mPostgresSaver._cursor\u001b[39m\u001b[34m(self, pipeline)\u001b[39m\n\u001b[32m    428\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m cur\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_factory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdict_row\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m cur:\n\u001b[32m    431\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m cur\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/psycopg/connection.py:231\u001b[39m, in \u001b[36mConnection.cursor\u001b[39m\u001b[34m(self, name, binary, row_factory, scrollable, withhold)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcursor\u001b[39m(\n\u001b[32m    220\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    221\u001b[39m     name: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    226\u001b[39m     withhold: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    227\u001b[39m ) -> Cursor[Any] | ServerCursor[Any]:\n\u001b[32m    228\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    229\u001b[39m \u001b[33;03m    Return a new `Cursor` to send commands and queries to the connection.\u001b[39;00m\n\u001b[32m    230\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_connection_ok\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m row_factory:\n\u001b[32m    234\u001b[39m         row_factory = \u001b[38;5;28mself\u001b[39m.row_factory\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/psycopg/_connection_base.py:528\u001b[39m, in \u001b[36mBaseConnection._check_connection_ok\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    527\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pgconn.status == BAD:\n\u001b[32m--> \u001b[39m\u001b[32m528\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.OperationalError(\u001b[33m\"\u001b[39m\u001b[33mthe connection is closed\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e.InterfaceError(\n\u001b[32m    530\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcannot execute operations: the connection is\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    531\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m in status \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.pgconn.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    532\u001b[39m )\n",
      "\u001b[31mOperationalError\u001b[39m: the connection is closed"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.types import Command\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# Initial invocation\n",
    "config = RunnableConfig({\"run_name\": \"Human_in_the_loop\", \"thread_id\": \"1\"})\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(\"Delete old records from the database\")],\n",
    "    },\n",
    "    config\n",
    ")\n",
    "\n",
    "# Check if paused for approval\n",
    "state = agent.get_state(config)\n",
    "if state.next:\n",
    "    request = state[\"__interrupt__\"].value[0][\"action_request\"]\n",
    "\n",
    "    # Display tool details to human\n",
    "    print(\"Tool:\", request[\"action\"])\n",
    "    print(\"Arguments:\", request[\"args\"])\n",
    "\n",
    "    # Resume with approval decision\n",
    "    agent.invoke(\n",
    "        Command(\n",
    "            resume=[{\"type\": \"accept\"}]  # or \"edit\", \"ignore\", \"response\"\n",
    "        ),\n",
    "        config=config\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57fe660",
   "metadata": {},
   "source": [
    "#### Human-in-the-loop wrapping tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from langchain_core.tools import BaseTool, tool as create_tool\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.types import interrupt\n",
    "from langgraph.prebuilt.interrupt import HumanInterruptConfig, HumanInterrupt\n",
    "\n",
    "def add_human_in_the_loop(\n",
    "    tool: Callable | BaseTool,\n",
    "    *,\n",
    "    interrupt_config: HumanInterruptConfig = None,\n",
    ") -> BaseTool:\n",
    "    \"\"\"Wrap a tool to support human-in-the-loop review.\"\"\"\n",
    "    if not isinstance(tool, BaseTool):\n",
    "        tool = create_tool(tool)\n",
    "\n",
    "    if interrupt_config is None:\n",
    "        interrupt_config = {\n",
    "            \"allow_accept\": True,\n",
    "            \"allow_edit\": True,\n",
    "            \"allow_respond\": True,\n",
    "        }\n",
    "\n",
    "    @create_tool(  # (1)!\n",
    "        tool.name,\n",
    "        description=tool.description,\n",
    "        args_schema=tool.args_schema\n",
    "    )\n",
    "    def call_tool_with_interrupt(config: RunnableConfig, **tool_input):\n",
    "        request: HumanInterrupt = {\n",
    "            \"action_request\": {\n",
    "                \"action\": tool.name,\n",
    "                \"args\": tool_input\n",
    "            },\n",
    "            \"config\": interrupt_config,\n",
    "            \"description\": \"Please review the tool call\"\n",
    "        }\n",
    "        response = interrupt([request])[0]  # (2)!\n",
    "        # approve the tool call\n",
    "        if response[\"type\"] == \"accept\":\n",
    "            tool_response = tool.invoke(tool_input, config)\n",
    "        # update tool call args\n",
    "        elif response[\"type\"] == \"edit\":\n",
    "            tool_input = response[\"args\"][\"args\"]\n",
    "            tool_response = tool.invoke(tool_input, config)\n",
    "        # respond to the LLM with user feedback\n",
    "        elif response[\"type\"] == \"response\":\n",
    "            user_feedback = response[\"args\"]\n",
    "            tool_response = user_feedback\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported interrupt response type: {response['type']}\")\n",
    "\n",
    "        return tool_response\n",
    "\n",
    "    return call_tool_with_interrupt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54ccac6",
   "metadata": {},
   "source": [
    "## Display LangGraph flow \n",
    "\n",
    "Also checkout: https://docs.langchain.com/oss/python/langchain/ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663b759",
   "metadata": {},
   "source": [
    "## Agent Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6e80b8",
   "metadata": {},
   "source": [
    "### Short Term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "301f18c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'v': 4, 'ts': '2025-09-26T09:41:53.691544+00:00', 'id': '1f09abd0-8e08-6c90-8004-90cab8933058', 'channel_versions': {'__start__': '00000000000000000000000000000005.0.7601503170391442', 'messages': '00000000000000000000000000000006.0.07850529529450567', 'branch:to:agent': '00000000000000000000000000000006.0.07850529529450567'}, 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.3017933414812394'}, 'agent': {'branch:to:agent': '00000000000000000000000000000005.0.7601503170391442'}}, 'updated_channels': ['messages'], 'channel_values': {'messages': [HumanMessage(content='Hi! My name is Bob.', additional_kwargs={}, response_metadata={}, id='601dc69b-16c9-4478-97e9-0b1b0ef20b52'), AIMessage(content='Hello Bob! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 30, 'total_tokens': 46, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CJzFYffTM8l0f0qRKlQAPuDYBEbtr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--8cd2ee7a-df86-4468-8bdd-6bcd05b77934-0', usage_metadata={'input_tokens': 30, 'output_tokens': 16, 'total_tokens': 46, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='Hello Bob! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 49, 'total_tokens': 65, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CJzFZzE89DNfq34u3iMQM4P29rssE', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--cee909cc-b08f-4e9e-8158-74021e21405c-0', usage_metadata={'input_tokens': 49, 'output_tokens': 16, 'total_tokens': 65, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}}\n",
      "StateSnapshot(values={'messages': [HumanMessage(content='Hi! My name is Bob.', additional_kwargs={}, response_metadata={}, id='601dc69b-16c9-4478-97e9-0b1b0ef20b52'), AIMessage(content='Hello Bob! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 30, 'total_tokens': 46, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CJzFYffTM8l0f0qRKlQAPuDYBEbtr', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--8cd2ee7a-df86-4468-8bdd-6bcd05b77934-0', usage_metadata={'input_tokens': 30, 'output_tokens': 16, 'total_tokens': 46, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), AIMessage(content='Hello Bob! Nice to meet you. How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 49, 'total_tokens': 65, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7c233bf9d1', 'id': 'chatcmpl-CJzFZzE89DNfq34u3iMQM4P29rssE', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--cee909cc-b08f-4e9e-8158-74021e21405c-0', usage_metadata={'input_tokens': 49, 'output_tokens': 16, 'total_tokens': 65, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f09abd0-8e08-6c90-8004-90cab8933058'}}, metadata={'source': 'loop', 'step': 4, 'parents': {}}, created_at='2025-09-26T09:41:53.691544+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f09abd0-86fd-6626-8003-7b6089dbc04b'}}, tasks=(), interrupts=())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "checkpointer = InMemorySaver() # Local\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "agent = create_agent(\n",
    "    model=select_model,\n",
    "    tools=[],\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "messages = {\"messages\": [HumanMessage(content=\"Hi! My name is Bob.\")]}\n",
    "agent.invoke(\n",
    "    messages,\n",
    "    config,\n",
    ")\n",
    "agent.invoke(\n",
    "    messages,\n",
    "    config,\n",
    ")\n",
    "\n",
    "check_point = checkpointer.get({\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "print(check_point)\n",
    "print(agent.get_state(config))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "286b7258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "UndefinedFunction",
     "evalue": "operator does not exist: text = smallint\nLINE 26: from checkpoints WHERE thread_id = $1 AND checkpoint_ns = $2...\n                                          ^\nHINT:  No operator matches the given name and argument types. You might need to add explicit type casts.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUndefinedFunction\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[96]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     18\u001b[39m thread_id = get_next_thread_id()\n\u001b[32m     19\u001b[39m agent.invoke(\n\u001b[32m     20\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mHi! My name is Bob.\u001b[39m\u001b[33m\"\u001b[39m}]},\n\u001b[32m     21\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mconfigurable\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mthread_id\u001b[39m\u001b[33m\"\u001b[39m: thread_id}}\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m check_point = \u001b[43mcheckpointer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfigurable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthread_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthread_id\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m print_dictionary(check_point)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langgraph/checkpoint/base/__init__.py:149\u001b[39m, in \u001b[36mBaseCheckpointSaver.get\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: RunnableConfig) -> Checkpoint | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    141\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fetch a checkpoint using the given configuration.\u001b[39;00m\n\u001b[32m    142\u001b[39m \n\u001b[32m    143\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m \u001b[33;03m        Optional[Checkpoint]: The requested checkpoint, or None if not found.\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value := \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m value.checkpoint\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langgraph/checkpoint/postgres/__init__.py:227\u001b[39m, in \u001b[36mPostgresSaver.get_tuple\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    224\u001b[39m     where = \u001b[33m\"\u001b[39m\u001b[33mWHERE thread_id = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m AND checkpoint_ns = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m ORDER BY checkpoint_id DESC LIMIT 1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cursor() \u001b[38;5;28;01mas\u001b[39;00m cur:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[43mcur\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mSELECT_SQL\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m     value = cur.fetchone()\n\u001b[32m    232\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/psycopg/cursor.py:97\u001b[39m, in \u001b[36mCursor.execute\u001b[39m\u001b[34m(self, query, params, prepare, binary)\u001b[39m\n\u001b[32m     93\u001b[39m         \u001b[38;5;28mself\u001b[39m._conn.wait(\n\u001b[32m     94\u001b[39m             \u001b[38;5;28mself\u001b[39m._execute_gen(query, params, prepare=prepare, binary=binary)\n\u001b[32m     95\u001b[39m         )\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m e._NO_TRACEBACK \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ex.with_traceback(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[31mUndefinedFunction\u001b[39m: operator does not exist: text = smallint\nLINE 26: from checkpoints WHERE thread_id = $1 AND checkpoint_ns = $2...\n                                          ^\nHINT:  No operator matches the given name and argument types. You might need to add explicit type casts."
     ]
    }
   ],
   "source": [
    "%pip install -q langgraph-checkpoint-postgres\n",
    "%pip install -q \"psycopg[binary,pool]\"\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "# python -m langgraph.checkpoint.postgres.migrate --url postgresql://postgres:postgres@localhost:5432/postgres\n",
    "\n",
    "\n",
    "DB_URI = \"postgresql://thomas:postgres@localhost:5432/postgres?sslmode=disable\"\n",
    "with PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n",
    "    checkpointer.setup()\n",
    "    model = create_model()\n",
    "    agent = create_agent(\n",
    "        model,\n",
    "        [],\n",
    "        checkpointer=checkpointer,\n",
    "    )\n",
    "\n",
    "    thread_id = get_next_thread_id()\n",
    "    agent.invoke(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]},\n",
    "        {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    )\n",
    "\n",
    "    check_point = checkpointer.get({\"configurable\": {\"thread_id\": thread_id}})\n",
    "    print_dictionary(check_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ba2cc09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "dict_keys(['messages'])\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Bob. How can I assist you further?\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'context'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[111]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(final_response.keys())\n\u001b[32m     42\u001b[39m final_response[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][-\u001b[32m1\u001b[39m].pretty_print()\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSummary:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mfinal_response\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mrunning_summary\u001b[39m\u001b[33m\"\u001b[39m].summary)\n",
      "\u001b[31mKeyError\u001b[39m: 'context'"
     ]
    }
   ],
   "source": [
    "%pip install -q langmem\n",
    "from langmem.short_term import SummarizationNode, RunningSummary\n",
    "from langchain_core.messages.utils import count_tokens_approximately\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "\n",
    "summarization_node = SummarizationNode(\n",
    "    token_counter=count_tokens_approximately,\n",
    "    model=model,\n",
    "    max_tokens=384,\n",
    "    max_summary_tokens=128,\n",
    "    output_messages_key=\"llm_input_messages\",\n",
    ")\n",
    "\n",
    "class State(AgentState):\n",
    "    # Added for the SummarizationNode to be able to keep track of the running summary information\n",
    "    context: dict[str, RunningSummary]\n",
    "\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    pre_model_hook=summarization_node,\n",
    "    state_schema=State,\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "config: RunnableConfig = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "agent.invoke({\"messages\": \"hi, my name is bob\"}, config)\n",
    "agent.invoke({\"messages\": \"write a short poem about cats\"}, config)\n",
    "agent.invoke({\"messages\": \"now do the same but for dogs\"}, config)\n",
    "final_response = agent.invoke({\"messages\": \"what's my name?\"}, config)\n",
    "\n",
    "print(final_response.keys())\n",
    "\n",
    "final_response[\"messages\"][-1].pretty_print()\n",
    "print(\"\\nSummary:\", final_response[\"context\"][\"running_summary\"].summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121f88cb",
   "metadata": {},
   "source": [
    "#### Long term Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e018ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "from langgraph.store.postgres import PostgresStore\n",
    "from langgraph.store.base import BaseStore\n",
    "\n",
    "model = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n",
    "\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "\n",
    "with (\n",
    "    PostgresStore.from_conn_string(DB_URI) as store,\n",
    "    PostgresSaver.from_conn_string(DB_URI) as checkpointer,\n",
    "):\n",
    "    # store.setup()\n",
    "    # checkpointer.setup()\n",
    "\n",
    "    def call_model(\n",
    "        state: MessagesState,\n",
    "        config: RunnableConfig,\n",
    "        *,\n",
    "        store: BaseStore,\n",
    "    ):\n",
    "        user_id = config[\"configurable\"][\"user_id\"]\n",
    "        namespace = (\"memories\", user_id)\n",
    "        memories = store.search(namespace, query=str(state[\"messages\"][-1].content))\n",
    "        info = \"\\n\".join([d.value[\"data\"] for d in memories])\n",
    "        system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n",
    "\n",
    "        # Store new memories if the user asks the model to remember\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        if \"remember\" in last_message.content.lower():\n",
    "            memory = \"User name is Bob\"\n",
    "            store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\n",
    "\n",
    "        response = model.invoke(\n",
    "            [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n",
    "        )\n",
    "        return {\"messages\": response}\n",
    "\n",
    "    builder = StateGraph(MessagesState)\n",
    "    builder.add_node(call_model)\n",
    "    builder.add_edge(START, \"call_model\")\n",
    "\n",
    "    graph = builder.compile(\n",
    "        checkpointer=checkpointer,\n",
    "        store=store,\n",
    "    )\n",
    "\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"1\",\n",
    "            \"user_id\": \"1\",\n",
    "        }\n",
    "    }\n",
    "    for chunk in graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        chunk[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"2\",\n",
    "            \"user_id\": \"1\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "    for chunk in graph.stream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n",
    "        config,\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59df09e8",
   "metadata": {},
   "source": [
    "#### Long term memory with semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898cdd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "from langchain.embeddings import init_embeddings\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\")\n",
    "\n",
    "# Create store with semantic search enabled\n",
    "embeddings = init_embeddings(\"openai:text-embedding-3-small\")\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embeddings,\n",
    "        \"dims\": 1536,\n",
    "    }\n",
    ")\n",
    "\n",
    "store.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\n",
    "store.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n",
    "\n",
    "def chat(state, *, store: BaseStore):\n",
    "    # Search based on user's last message\n",
    "    items = store.search(\n",
    "        (\"user_123\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\n",
    "    )\n",
    "    memories = \"\\n\".join(item.value[\"text\"] for item in items)\n",
    "    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"},\n",
    "            *state[\"messages\"],\n",
    "        ]\n",
    "    )\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(chat)\n",
    "builder.add_edge(START, \"chat\")\n",
    "graph = builder.compile(store=store)\n",
    "\n",
    "for message, metadata in graph.stream(\n",
    "    input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]},\n",
    "    stream_mode=\"messages\",\n",
    "):\n",
    "    print(message.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681b165f",
   "metadata": {},
   "source": [
    "## Multi-agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3125226",
   "metadata": {},
   "source": [
    "### Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "470e9e58",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3163895073.py, line 4)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[114]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31msubagent1 = create_agent(..)\u001b[39m\n                             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "subagent1 = create_agent(..)\n",
    "\n",
    "@tool(\n",
    "    name=\"subagent1_name\",\n",
    "    description=\"subagent1_description\"\n",
    ")\n",
    "def call_subagent1(query: str):\n",
    "    result = subagent1.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "    })\n",
    "    return result[\"messages\"].text\n",
    "\n",
    "agent = create_agent(..., tools=[call_subagent1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c75f7af",
   "metadata": {},
   "source": [
    "#### Validate input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e87833cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tool() got an unexpected keyword argument 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AgentState\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magents\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtool_node\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InjectedState\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;129m@tool\u001b[39m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msubagent1_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msubagent1_description\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_subagent1\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m, state: Annotated[CustomState, InjectedState]):\n\u001b[32m     10\u001b[39m     \u001b[38;5;66;03m# Apply any logic needed to transform the messages into a suitable input\u001b[39;00m\n\u001b[32m     11\u001b[39m     subagent_input = some_logic(query, state.messages)\n\u001b[32m     12\u001b[39m     result = subagent1.invoke({\n\u001b[32m     13\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: subagent_input,\n\u001b[32m     14\u001b[39m         \u001b[38;5;66;03m# You could also pass other state keys here as needed.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mexample_state_key\u001b[39m\u001b[33m\"\u001b[39m: state.example_state_key\n\u001b[32m     18\u001b[39m     })\n",
      "\u001b[31mTypeError\u001b[39m: tool() got an unexpected keyword argument 'name'"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "from langchain.agents import AgentState\n",
    "from langchain.agents.tool_node import InjectedState\n",
    "\n",
    "@tool(\n",
    "    name=\"subagent1_name\",\n",
    "    description=\"subagent1_description\"\n",
    ")\n",
    "def call_subagent1(query: str, state: Annotated[CustomState, InjectedState]):\n",
    "    # Apply any logic needed to transform the messages into a suitable input\n",
    "    subagent_input = some_logic(query, state.messages)\n",
    "    result = subagent1.invoke({\n",
    "        \"messages\": subagent_input,\n",
    "        # You could also pass other state keys here as needed.\n",
    "        # Make sure to define these in both the main and subagent's\n",
    "        # state schemas.\n",
    "        \"example_state_key\": state.example_state_key\n",
    "    })\n",
    "    return result[\"messages\"][-1].text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688535bf",
   "metadata": {},
   "source": [
    "#### Format output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d58747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain.agents import AgentState\n",
    "from langchain_core.tools import InjectedToolCallId\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "@tool(\n",
    "    name=\"subagent1_name\",\n",
    "    description=\"subagent1_description\"\n",
    ")\n",
    "# We need to pass the `tool_call_id` to the sub agent so it can use it to respond with the tool call result\n",
    "def call_subagent1(\n",
    "    query: str,\n",
    "    tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "# You need to return a `Command` object to include more than just a final tool call\n",
    ") -> Command:\n",
    "    result = subagent1.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "    })\n",
    "    return Command(update={\n",
    "        # This is the example state key we are passing back\n",
    "        \"example_state_key\": result[\"example_state_key\"],\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                # We need to include the tool call id so it matches up with the right tool call\n",
    "                result[\"messages\"][-1].text, tool_call_id=tool_call_id\n",
    "            )\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55517cf9",
   "metadata": {},
   "source": [
    "## MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15660486",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"langgraph-api>=0.2.3\" \"langgraph-sdk>=0.1.61\"\n",
    "%pip install -q langchain-mcp-adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b0e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create server parameters for stdio connection\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "import asyncio\n",
    "\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "server_params = {\n",
    "    \"url\": \"https://mcp-finance-agent.xxx.us.langgraph.app/mcp\",\n",
    "    \"headers\": {\n",
    "        \"X-Api-Key\":\"lsv2_pt_your_api_key\"\n",
    "    }\n",
    "}\n",
    "\n",
    "async def main():\n",
    "    async with streamablehttp_client(**server_params) as (read, write, _):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize the connection\n",
    "            await session.initialize()\n",
    "\n",
    "            # Load the remote graph as if it was a tool\n",
    "            tools = await load_mcp_tools(session)\n",
    "\n",
    "            # Create and run a react agent with the tools\n",
    "            agent = create_react_agent(\"openai:gpt-4.1\", tools)\n",
    "\n",
    "            # Invoke the agent with a message\n",
    "            query = create_initial_message(\"What can the finance agent do for me?\")\n",
    "            agent_response = await agent.ainvoke(query)\n",
    "            print(agent_response)\n",
    "            print_dictionary(agent_response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffe609",
   "metadata": {},
   "source": [
    "### Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "63d41e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='add', description='Add two numbers', args_schema={'type': 'object', 'properties': {'a': {'type': 'number', 'description': 'First number'}, 'b': {'type': 'number', 'description': 'Second number'}}, 'required': ['a', 'b']}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x1119c4180>), StructuredTool(name='subtract', description='Subtract two numbers', args_schema={'type': 'object', 'properties': {'a': {'type': 'number', 'description': 'First number'}, 'b': {'type': 'number', 'description': 'Second number'}}, 'required': ['a', 'b']}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x11209cf40>), StructuredTool(name='multiply', description='Multiply two numbers', args_schema={'type': 'object', 'properties': {'a': {'type': 'number', 'description': 'First number'}, 'b': {'type': 'number', 'description': 'Second number'}}, 'required': ['a', 'b']}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x11209c0e0>), StructuredTool(name='divide', description='Divide two numbers', args_schema={'type': 'object', 'properties': {'a': {'type': 'number', 'description': 'Dividend'}, 'b': {'type': 'number', 'description': 'Divisor'}}, 'required': ['a', 'b']}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x11209e160>), StructuredTool(name='get_weather', description='Get the current weather for a specified location.', args_schema={'properties': {'location': {'title': 'Location', 'type': 'string'}}, 'required': ['location'], 'type': 'object'}, metadata={'_meta': {'version': '1.0', 'author': 'weather-team', '_fastmcp': {'tags': ['forecast', 'weather']}}}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x11209f600>)]\n",
      "Message 1:\n",
      "    Role: human\n",
      "    Content: what's (3 + 5) x 12?\n",
      "\n",
      "Message 2:\n",
      "    Role: ai\n",
      "    Content: \n",
      "\n",
      "    Metadata: {'token_usage': {'completion_tokens': 50, 'prompt_tokens': 181, 'total_tokens': 231, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_04d3664870', 'id': 'chatcmpl-CK20cCZxWjubdODedwYFgB83p820x', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}\n",
      "\n",
      "    Additional kwargs: {'tool_calls': [{'id': 'call_SYpGPHTfPAHLARVyxkotHih7', 'function': {'arguments': '{\"a\": 3, \"b\": 5}', 'name': 'add'}, 'type': 'function'}, {'id': 'call_7EZUP9ic8LRdzxtpxS8obZZt', 'function': {'arguments': '{\"a\": 12, \"b\": 1}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None}\n",
      "\n",
      "    Usage metadata: {'input_tokens': 181, 'output_tokens': 50, 'total_tokens': 231, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Message 3:\n",
      "    Role: tool\n",
      "    Content: {\n",
      "  \"result\": 8\n",
      "}\n",
      "\n",
      "Message 4:\n",
      "    Role: tool\n",
      "    Content: {\n",
      "  \"result\": 12\n",
      "}\n",
      "\n",
      "Message 5:\n",
      "    Role: ai\n",
      "    Content: The sum of 3 and 5 is 8. Multiplying 12 by 1 gives 12. The expression (3 + 5) x 12 equals 8 x 12, which is 96.\n",
      "\n",
      "    Metadata: {'token_usage': {'completion_tokens': 47, 'prompt_tokens': 254, 'total_tokens': 301, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_04d3664870', 'id': 'chatcmpl-CK20fBTFg4whaw7g3oP6jSjS0ZBPb', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}\n",
      "\n",
      "    Additional kwargs: {'refusal': None}\n",
      "\n",
      "    Usage metadata: {'input_tokens': 254, 'output_tokens': 47, 'total_tokens': 301, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Message 1:\n",
      "    Role: human\n",
      "    Content: what is the weather in nyc?\n",
      "\n",
      "Message 2:\n",
      "    Role: ai\n",
      "    Content: \n",
      "\n",
      "    Metadata: {'token_usage': {'completion_tokens': 15, 'prompt_tokens': 177, 'total_tokens': 192, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_04d3664870', 'id': 'chatcmpl-CK21CDyleuJ3eeOWKCjQiNZYcCKXI', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}\n",
      "\n",
      "    Additional kwargs: {'tool_calls': [{'id': 'call_KBr1rK5rTsf99UOX5q0085HW', 'function': {'arguments': '{\"location\":\"NYC\"}', 'name': 'get_weather'}, 'type': 'function'}], 'refusal': None}\n",
      "\n",
      "    Usage metadata: {'input_tokens': 177, 'output_tokens': 15, 'total_tokens': 192, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "Message 3:\n",
      "    Role: tool\n",
      "    Content: {\"location\":\"NYC\",\"temperature\":\"18°C\",\"condition\":\"Partly Cloudy\"}\n",
      "\n",
      "Message 4:\n",
      "    Role: ai\n",
      "    Content: The weather in NYC is partly cloudy with a temperature of 18°C.\n",
      "\n",
      "    Metadata: {'token_usage': {'completion_tokens': 16, 'prompt_tokens': 218, 'total_tokens': 234, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_04d3664870', 'id': 'chatcmpl-CK21DkkyMkLBiloyQxbzPFkOyusx8', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}\n",
      "\n",
      "    Additional kwargs: {'refusal': None}\n",
      "\n",
      "    Usage metadata: {'input_tokens': 218, 'output_tokens': 16, 'total_tokens': 234, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "path_math = \"/Users/thomas/Desktop/Agentic-AI/LangChain/math_server.py\"\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"math\": {\n",
    "            \"transport\": \"stdio\",  # Local subprocess communication\n",
    "            \"command\": \"python\",\n",
    "            # Absolute path to your math_server.py file\n",
    "            \"args\": [path_math],\n",
    "        },\n",
    "        \"weather\": {\n",
    "            \"transport\": \"streamable_http\",  # HTTP-based remote server\n",
    "            # Ensure you start your weather server on port 8000\n",
    "            \"url\": \"http://localhost:8000/mcp\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()\n",
    "\n",
    "print(tools)\n",
    "model = create_model()\n",
    "agent = create_agent(\n",
    "    model, \n",
    "    tools=tools,\n",
    "    prompt=SystemMessage(content=\"You are a personal assistant. That can use tools\")\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "math_request = create_initial_message(\"what's (3 + 5) x 12?\")\n",
    "\n",
    "math_response = await agent.ainvoke(math_request, thread_id=\"session_1\")\n",
    "\n",
    "weather_request = create_initial_message(\"what is the weather in nyc?\")\n",
    "weather_response = await agent.ainvoke(weather_request,thread_id=\"session_2\")\n",
    "\n",
    "\n",
    "print_dictionary(math_response)\n",
    "print_dictionary(weather_response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16000e3b",
   "metadata": {},
   "source": [
    "### MCP toolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacff03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "def mcp_tools_node(state, config):\n",
    "    user = config[\"configurable\"].get(\"langgraph_auth_user\")\n",
    "         , user[\"github_token\"], user[\"email\"], etc.\n",
    "\n",
    "    client = MultiServerMCPClient({\n",
    "        \"github\": {\n",
    "            \"transport\": \"streamable_http\", # (1)\n",
    "            \"url\": \"https://my-github-mcp-server/mcp\", # (2)\n",
    "            \"headers\": {\n",
    "                \"Authorization\": f\"Bearer {user['github_token']}\"\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    tools = await client.get_tools() # (3)\n",
    "\n",
    "    # Your tool-calling logic here\n",
    "\n",
    "    tool_messages = \n",
    "    return {\"messages\": tool_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f19aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mcp\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

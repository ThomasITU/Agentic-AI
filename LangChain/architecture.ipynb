{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b2784ac",
   "metadata": {},
   "source": [
    "# OSS (LangChain) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8ddfdc",
   "metadata": {},
   "source": [
    "## Install and helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940d5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langchain-core\n",
    "%pip install -q langchain-openai\n",
    "%pip install -q langchain-experimental\n",
    "%pip install --pre -U -q langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ce091a",
   "metadata": {},
   "source": [
    "### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "366e4a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty print\n",
    "\n",
    "def pretty_print(response:dict):\n",
    "    messages = response[\"messages\"]\n",
    "    for message in messages:\n",
    "        print(message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ccb9ed",
   "metadata": {},
   "source": [
    "## Simple processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "280a9dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1-nano\", model_provider=\"openai\")\n",
    "\n",
    "query = input()\n",
    "if query:\n",
    "  result = model.invoke(query)\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e861c0e",
   "metadata": {},
   "source": [
    "## Simple Agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d3a94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the weather in copenhagen\n",
      "I do not have access to real-time data, so I cannot provide the current weather in Copenhagen. For the most up-to-date weather information, please consult a reliable weather service such as Weather.com, AccuWeather, or a weather app on your device.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "tools = []\n",
    "agent = create_agent(\n",
    "    model, \n",
    "    tools=tools,\n",
    "    prompt=SystemMessage(content=\"You are a research assistant. Cite your sources.\")\n",
    ")\n",
    "\n",
    "\n",
    "query = input()\n",
    "if query:\n",
    "    result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
    "\n",
    "    pretty_print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392f890a",
   "metadata": {},
   "source": [
    "#### Choose model dynamicly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf2d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent, AgentState\n",
    "from langgraph.runtime import Runtime\n",
    "\n",
    "def select_model(state: AgentState, runtime: Runtime) -> ChatOpenAI:\n",
    "    \"\"\"Choose model based on conversation complexity.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    message_count = len(messages)\n",
    "\n",
    "    if message_count < 10:\n",
    "        return ChatOpenAI(model=\"gpt-4.1-nano\").bind_tools(tools)\n",
    "    else:\n",
    "        return ChatOpenAI(model=\"gpt-3.5\").bind_tools(tools) # Better model for longer conversations\n",
    "\n",
    "agent = create_agent(select_model, tools=tools)\n",
    "\n",
    "\n",
    "print(agent.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcae014",
   "metadata": {},
   "source": [
    "#### Set prompt Dynamicly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53227bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_prompt(state):\n",
    "    user_type = state.get(\"user_type\", \"standard\")\n",
    "    system_msg = SystemMessage(\n",
    "        content=\"Provide detailed technical responses.\" if user_type == \"expert\" else \"Provide simple, clear explanations.\"\n",
    "    )\n",
    "    return [system_msg] + state[\"messages\"]\n",
    "agent = create_agent(model, tools, prompt=dynamic_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86202fd9",
   "metadata": {},
   "source": [
    "#### Multi-turn conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b879fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import filter_messages\n",
    "\n",
    "def manage_conversation_window(messages, max_messages=10):\n",
    "    \"\"\"Keep only the system message and last N messages\"\"\"\n",
    "    system_msgs = filter_messages(messages, include_types=\"system\")\n",
    "    recent_msgs = messages[-(max_messages-len(system_msgs)):]\n",
    "    return system_msgs + recent_msgs\n",
    "\n",
    "messages = [SystemMessage(content=\"Your a helpful assistant\")]\n",
    "\n",
    "# Usage in conversation loop\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    messages.append(HumanMessage(content=user_input))\n",
    "\n",
    "    # Trim conversation to fit context window\n",
    "    messages = manage_conversation_window(messages)\n",
    "\n",
    "    response = model.invoke(messages)\n",
    "    messages.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dab5a2",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8762da6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for information.\"\"\"\n",
    "    return f\"Results for: {query}\"\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Perform calculations.\"\"\"\n",
    "    return str(eval(expression))\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_database(query: str, limit: int = 10) -> str:\n",
    "    \"\"\"Search the customer database for records matching the query.\n",
    "\n",
    "    Args:\n",
    "        query: Search terms to look for\n",
    "        limit: Maximum number of results to return\n",
    "    \"\"\"\n",
    "    return f\"Found {limit} results for '{query}'\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad7fa6",
   "metadata": {},
   "source": [
    "### ToolNode for error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ad0b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsupported message type: <class 'ellipsis'>\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      3\u001b[39m tool_node = ToolNode(\n\u001b[32m      4\u001b[39m     tools=[search, calculate],\n\u001b[32m      5\u001b[39m     handle_tool_errors=\u001b[33m\"\u001b[39m\u001b[33mPlease check your input and try again.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m agent = create_agent(model, tools=tool_node)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m result = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:3026\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3024\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3026\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3031\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3032\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3034\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3035\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3036\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3038\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3039\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3040\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langgraph/pregel/main.py:2657\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2647\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.tick(\n\u001b[32m   2648\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2649\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2652\u001b[39m ):\n\u001b[32m   2653\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2654\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[32m   2655\u001b[39m         stream_mode, print_mode, subgraphs, stream.get, queue.Empty\n\u001b[32m   2656\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2657\u001b[39m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mafter_tick\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2658\u001b[39m \u001b[38;5;66;03m# wait for checkpoint\u001b[39;00m\n\u001b[32m   2659\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m durability_ == \u001b[33m\"\u001b[39m\u001b[33msync\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langgraph/pregel/_loop.py:525\u001b[39m, in \u001b[36mPregelLoop.after_tick\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    523\u001b[39m writes = [w \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tasks.values() \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m t.writes]\n\u001b[32m    524\u001b[39m \u001b[38;5;66;03m# all tasks have finished\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m \u001b[38;5;28mself\u001b[39m.updated_channels = \u001b[43mapply_writes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    526\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    527\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    529\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcheckpointer_get_next_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    530\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrigger_to_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# produce values output\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.updated_channels.isdisjoint(\n\u001b[32m    534\u001b[39m     (\u001b[38;5;28mself\u001b[39m.output_keys,)\n\u001b[32m    535\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.output_keys, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output_keys\n\u001b[32m    537\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langgraph/pregel/_algo.py:295\u001b[39m, in \u001b[36mapply_writes\u001b[39m\u001b[34m(checkpoint, channels, tasks, get_next_version, trigger_to_nodes)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chan, vals \u001b[38;5;129;01min\u001b[39;00m pending_writes_by_channel.items():\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chan \u001b[38;5;129;01min\u001b[39;00m channels:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mchannels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchan\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m next_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    296\u001b[39m             checkpoint[\u001b[33m\"\u001b[39m\u001b[33mchannel_versions\u001b[39m\u001b[33m\"\u001b[39m][chan] = next_version\n\u001b[32m    297\u001b[39m             \u001b[38;5;66;03m# unavailable channels can't trigger tasks, so don't add them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langgraph/channels/binop.py:93\u001b[39m, in \u001b[36mBinaryOperatorAggregate.update\u001b[39m\u001b[34m(self, values)\u001b[39m\n\u001b[32m     91\u001b[39m     values = values[\u001b[32m1\u001b[39m:]\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m values:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28mself\u001b[39m.value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moperator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langgraph/graph/message.py:47\u001b[39m, in \u001b[36m_add_messages_wrapper.<locals>._add_messages\u001b[39m\u001b[34m(left, right, **kwargs)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_add_messages\u001b[39m(\n\u001b[32m     44\u001b[39m     left: Messages | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, right: Messages | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m, **kwargs: Any\n\u001b[32m     45\u001b[39m ) -> Messages | Callable[[Messages, Messages], Messages]:\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     49\u001b[39m         msg = (\n\u001b[32m     50\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMust specify non-null arguments for both \u001b[39m\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mright\u001b[39m\u001b[33m'\u001b[39m\u001b[33m. Only \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     51\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mreceived: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mleft\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mright\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     52\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langgraph/graph/message.py:190\u001b[39m, in \u001b[36madd_messages\u001b[39m\u001b[34m(left, right, format)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# coerce to message\u001b[39;00m\n\u001b[32m    184\u001b[39m left = [\n\u001b[32m    185\u001b[39m     message_chunk_to_message(cast(BaseMessageChunk, m))\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m convert_to_messages(left)\n\u001b[32m    187\u001b[39m ]\n\u001b[32m    188\u001b[39m right = [\n\u001b[32m    189\u001b[39m     message_chunk_to_message(cast(BaseMessageChunk, m))\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mconvert_to_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m ]\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# assign missing ids\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m left:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langchain_core/messages/utils.py:373\u001b[39m, in \u001b[36mconvert_to_messages\u001b[39m\u001b[34m(messages)\u001b[39m\n\u001b[32m    371\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages, PromptValue):\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m messages.to_messages()\n\u001b[32m--> \u001b[39m\u001b[32m373\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/.venv/lib/python3.13/site-packages/langchain_core/messages/utils.py:352\u001b[39m, in \u001b[36m_convert_to_message\u001b[39m\u001b[34m(message)\u001b[39m\n\u001b[32m    350\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported message type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    351\u001b[39m     msg = create_message(message=msg, error_code=ErrorCode.MESSAGE_COERCION_FAILURE)\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m message_\n",
      "\u001b[31mNotImplementedError\u001b[39m: Unsupported message type: <class 'ellipsis'>\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/MESSAGE_COERCION_FAILURE "
     ]
    }
   ],
   "source": [
    "from langchain.agents import ToolNode\n",
    "\n",
    "tool_node = ToolNode(\n",
    "    tools=[search, calculate],\n",
    "    handle_tool_errors=\"Please check your input and try again.\"\n",
    ")\n",
    "agent = create_agent(model, tools=tool_node)\n",
    "\n",
    "query = input()\n",
    "if query:\n",
    "    result = agent.invoke(query)\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54eb007",
   "metadata": {},
   "source": [
    "### Setup chromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbfa94e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "{'ids': [['id2', 'id1']], 'embeddings': None, 'documents': [['This is a document about oranges', 'This is a document about pineapple']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[None, None]], 'distances': [[1.1462141275405884, 1.3015384674072266]]}\n"
     ]
    }
   ],
   "source": [
    "%pip install -q chromadb\n",
    "\n",
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# switch `create_collection` to `get_or_create_collection` to avoid creating a new collection every time\n",
    "collection = chroma_client.get_or_create_collection(name=\"my_collection\")\n",
    "\n",
    "# switch `add` to `upsert` to avoid adding the same documents every time\n",
    "collection.upsert(\n",
    "    documents=[\n",
    "        \"This is a document about pineapple\",\n",
    "        \"This is a document about oranges\"\n",
    "    ],\n",
    "    ids=[\"id1\", \"id2\"]\n",
    ")\n",
    "\n",
    "results = collection.query(\n",
    "    query_texts=[\"This is a query document about florida\"], # Chroma will embed this for you\n",
    "    n_results=2 # how many results to return\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e6e49e",
   "metadata": {},
   "source": [
    "### RAG - Create agent Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd77e19",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3764591442.py, line 10)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mresponse =\u001b[39m\n               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-chroma\n",
    "\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Load the document, split it into chunks, embed each chunk and load it into the vector store.\n",
    "raw_documents = TextLoader('state_of_the_union.txt').load()\n",
    "\n",
    "query = input()\n",
    "\n",
    "response = model.invoke(query)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection_name = \"\"\n",
    "chroma_client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "db = Chroma.from_documents(documents, OpenAIEmbeddings())\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd2f131",
   "metadata": {},
   "source": [
    "### Create In memory vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89d7583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c8a0b3",
   "metadata": {},
   "source": [
    "### Human in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef974bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "def write_file_tool():\n",
    "    return \"succes\"\n",
    "\n",
    "def execute_sql_tool():\n",
    "    return \"succes\"\n",
    "\n",
    "def read_data_tool():\n",
    "    return \"succes\"\n",
    "\n",
    "\n",
    "tool_cfg = tool_configs={\n",
    "                \"write_file\": {\n",
    "                    \"allow_accept\": True,\n",
    "                    \"allow_edit\": True,\n",
    "                    \"allow_respond\": True,\n",
    "                    \"description\": \"⚠️ File write operation requires approval\",\n",
    "                },\n",
    "                \"execute_sql\": {\n",
    "                    \"allow_accept\": True,\n",
    "                    \"description\": \"🚨 SQL execution requires careful review\",\n",
    "                },\n",
    "                \"read_data\": False,  # Safe operation, no approval needed\n",
    "            }\n",
    "\n",
    "hitl = HumanInTheLoopMiddleware(tool_cfg, description_prefix=\"Tool execution pending approval\",)\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4o\",\n",
    "    tools=[write_file_tool, execute_sql_tool, read_data_tool],\n",
    "    middleware=[hitl],\n",
    "    checkpointer=InMemorySaver(),  # Required for interrupts\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.types import Command\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# Initial invocation\n",
    "config = RunnableConfig({\"run_name\": \"Human_in_the_loop\"})\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(\"Delete old records from the database\")],\n",
    "    },\n",
    "    config\n",
    ")\n",
    "\n",
    "# Check if paused for approval\n",
    "state = agent.get_state(config)\n",
    "if state.next:\n",
    "    request = state[\"__interrupt__\"].value[0][\"action_request\"]\n",
    "\n",
    "    # Display tool details to human\n",
    "    print(\"Tool:\", request[\"action\"])\n",
    "    print(\"Arguments:\", request[\"args\"])\n",
    "\n",
    "    # Resume with approval decision\n",
    "    agent.invoke(\n",
    "        Command(\n",
    "            resume=[{\"type\": \"accept\"}]  # or \"edit\", \"ignore\", \"response\"\n",
    "        ),\n",
    "        config=config\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54ccac6",
   "metadata": {},
   "source": [
    "### Display LangGraph flow \n",
    "\n",
    "Also checkout: https://docs.langchain.com/oss/python/langchain/ui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d3f01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663b759",
   "metadata": {},
   "source": [
    "## Agent Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6e80b8",
   "metadata": {},
   "source": [
    "### Short Term memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "checkpointer = InMemorySaver() # Local\n",
    "\n",
    "get_user_info = {\"info\":\"cool\"}\n",
    "\n",
    "agent = create_agent(\n",
    "    \"openai:gpt-4.1-nano\",\n",
    "    [get_user_info],\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]},\n",
    "    {\"configurable\": {\"thread_id\": \"1\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286b7258",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \n",
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "\n",
    "DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n",
    "with PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n",
    "    agent = create_agent(\n",
    "        \"openai:gpt-5\",\n",
    "        [get_user_info],\n",
    "        checkpointer=checkpointer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55517cf9",
   "metadata": {},
   "source": [
    "## MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15660486",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"langgraph-api>=0.2.3\" \"langgraph-sdk>=0.1.61\"\n",
    "%pip install -q langchain-mcp-adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b0e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create server parameters for stdio connection\n",
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "import asyncio\n",
    "\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "server_params = {\n",
    "    \"url\": \"https://mcp-finance-agent.xxx.us.langgraph.app/mcp\",\n",
    "    \"headers\": {\n",
    "        \"X-Api-Key\":\"lsv2_pt_your_api_key\"\n",
    "    }\n",
    "}\n",
    "\n",
    "async def main():\n",
    "    async with streamablehttp_client(**server_params) as (read, write, _):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            # Initialize the connection\n",
    "            await session.initialize()\n",
    "\n",
    "            # Load the remote graph as if it was a tool\n",
    "            tools = await load_mcp_tools(session)\n",
    "\n",
    "            # Create and run a react agent with the tools\n",
    "            agent = create_react_agent(\"openai:gpt-4.1\", tools)\n",
    "\n",
    "            # Invoke the agent with a message\n",
    "            agent_response = await agent.ainvoke({\"messages\": \"What can the finance agent do for me?\"})\n",
    "            print(agent_response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ffe609",
   "metadata": {},
   "source": [
    "### Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d41e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain.agents import create_agent\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "path_math = \"/Users/thomas/Desktop/Agentic-AI/LangChain/math_server.py\"\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"math\": {\n",
    "            \"transport\": \"stdio\",  # Local subprocess communication\n",
    "            \"command\": \"python\",\n",
    "            # Absolute path to your math_server.py file\n",
    "            \"args\": [path_math],\n",
    "        },\n",
    "        \"weather\": {\n",
    "            \"transport\": \"streamable_http\",  # HTTP-based remote server\n",
    "            # Ensure you start your weather server on port 8000\n",
    "            \"url\": \"http://localhost:8000/mcp\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model, \n",
    "    tools=tools,\n",
    "    prompt=SystemMessage(content=\"You are a personal assistant.\")\n",
    ")\n",
    "\n",
    "\n",
    "math_response = await agent.ainvoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]}\n",
    ")\n",
    "weather_response = await agent.ainvoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in nyc?\"}]}\n",
    ")\n",
    "\n",
    "\n",
    "pretty_print(math_response)\n",
    "pretty_print(weather_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16000e3b",
   "metadata": {},
   "source": [
    "### MCP toolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacff03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "def mcp_tools_node(state, config):\n",
    "    user = config[\"configurable\"].get(\"langgraph_auth_user\")\n",
    "         , user[\"github_token\"], user[\"email\"], etc.\n",
    "\n",
    "    client = MultiServerMCPClient({\n",
    "        \"github\": {\n",
    "            \"transport\": \"streamable_http\", # (1)\n",
    "            \"url\": \"https://my-github-mcp-server/mcp\", # (2)\n",
    "            \"headers\": {\n",
    "                \"Authorization\": f\"Bearer {user['github_token']}\"\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    tools = await client.get_tools() # (3)\n",
    "\n",
    "    # Your tool-calling logic here\n",
    "\n",
    "    tool_messages = ...\n",
    "    return {\"messages\": tool_messages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f19aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mcp\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

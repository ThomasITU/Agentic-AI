{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c025c5d",
   "metadata": {},
   "source": [
    "# Smol agents\n",
    "\n",
    "These agents try to solve problems using code and the tools you provide - they can be used in similar fashion as any other code and orchestrated as such   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6932c6b",
   "metadata": {},
   "source": [
    "## install and helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32d26a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade -q pip\n",
    "%pip install -q ipywidgets\n",
    "%pip install -q \"smolagents[toolkit]\" \n",
    "%pip install -q \"smolagents[litellm]\"\n",
    "%pip install -q python-dotenv\n",
    "%pip install -q \"smolagents[e2b]\"\n",
    "\n",
    "from smolagents import ToolCallingAgent, WebSearchTool, VisitWebpageTool, tool, CodeAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2347db8",
   "metadata": {},
   "source": [
    "#### System prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e02a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "better_prompt = '''\n",
    "    You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.\n",
    "    To do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.\n",
    "    To solve the task, you must plan forward to proceed in a series of steps, in a cycle of 'Thought:', 'Code:', and 'Observation:' sequences.\n",
    "\n",
    "    At each step, in the 'Thought:' attribute, you should first explain your reasoning towards solving the task and the tools that you want to use.\n",
    "    Then in the 'Code' attribute, you should write the code in simple Python.\n",
    "    During each intermediate step, you can use 'print()' to save whatever important information you will then need.\n",
    "    These print outputs will then appear in the 'Observation:' field, which will be available as input for the next step.\n",
    "    In the end you have to return a final answer using the `final_answer` tool. You will be generating a JSON object with the following structure:\n",
    "    ```json\n",
    "    {\n",
    "    \"thought\": \"...\",\n",
    "    \"code\": \"...\"\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Here are a few examples using notional tools:\n",
    "    ---\n",
    "    Task: \"Generate an image of the oldest person in this document.\"\n",
    "\n",
    "    {\"thought\": \"I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\", \"code\": \"answer = document_qa(document=document, question=\\\"Who is the oldest person mentioned?\\\")\\nprint(answer)\\n\"}\n",
    "    Observation: \"The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland.\"\n",
    "\n",
    "    {\"thought\": \"I will now generate an image showcasing the oldest person.\", \"code\": \"image = image_generator(\\\"A portrait of John Doe, a 55-year-old man living in Canada.\\\")\\nfinal_answer(image)\\n\"}\n",
    "    ---\n",
    "    Task: \"What is the result of the following operation: 5 + 3 + 1294.678?\"\n",
    "\n",
    "    {\"thought\": \"I will use python code to compute the result of the operation and then return the final answer using the `final_answer` tool\", \"code\": \"result = 5 + 3 + 1294.678\\nfinal_answer(result)\\n\"}\n",
    "\n",
    "    ---\n",
    "    Task:\n",
    "    In a 1979 interview, Stanislaus Ulam discusses with Martin Sherwin about other great physicists of his time, including Oppenheimer.\n",
    "    What does he say was the consequence of Einstein learning too much math on his creativity, in one word?\n",
    "\n",
    "    {\"thought\": \"I need to find and read the 1979 interview of Stanislaus Ulam with Martin Sherwin.\", \"code\": \"pages = web_search(query=\\\"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\\\")\\nprint(pages)\\n\"}\n",
    "    Observation:\n",
    "    No result found for query \"1979 interview Stanislaus Ulam Martin Sherwin physicists Einstein\".\n",
    "\n",
    "    {\"thought\": \"The query was maybe too restrictive and did not find any results. Let's try again with a broader query.\", \"code\": \"pages = web_search(query=\\\"1979 interview Stanislaus Ulam\\\")\\nprint(pages)\\n\"}\n",
    "    Observation:\n",
    "    Found 6 pages:\n",
    "    [Stanislaus Ulam 1979 interview](https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/)\n",
    "\n",
    "    [Ulam discusses Manhattan Project](https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/)\n",
    "\n",
    "    (truncated)\n",
    "\n",
    "    {\"thought\": \"I will read the first 2 pages to know more.\", \"code\": \"for url in [\\\"https://ahf.nuclearmuseum.org/voices/oral-histories/stanislaus-ulams-interview-1979/\\\", \\\"https://ahf.nuclearmuseum.org/manhattan-project/ulam-manhattan-project/\\\"]:\\n      whole_page = visit_webpage(url)\\n      print(whole_page)\\n      print(\\\"\\n\\\" + \\\"=\\\"*80 + \\\"\\n\\\")  # Print separator between pages\"}\n",
    "\n",
    "    Observation:\n",
    "    Manhattan Project Locations:\n",
    "    Los Alamos, NM\n",
    "    Stanislaus Ulam was a Polish-American mathematician. He worked on the Manhattan Project at Los Alamos and later helped design the hydrogen bomb. In this interview, he discusses his work at\n",
    "    (truncated)\n",
    "\n",
    "    {\"thought\": \"I now have the final answer: from the webpages visited, Stanislaus Ulam says of Einstein: \\\"He learned too much mathematics and sort of diminished, it seems to me personally, it seems to me his purely physics creativity.\\\" Let's answer in one word.\", \"code\": \"final_answer(\\\"diminished\\\")\"}\n",
    "\n",
    "    ---\n",
    "    Task: \"Which city has the highest population: Guangzhou or Shanghai?\"\n",
    "\n",
    "    {\"thought\": \"I need to get the populations for both cities and compare them: I will use the tool `web_search` to get the population of both cities.\", \"code\": \"for city in [\\\"Guangzhou\\\", \\\"Shanghai\\\"]:\\n      print(f\\\"Population {city}:\\\", web_search(f\\\"{city} population\\\")\"}\n",
    "    Observation:\n",
    "    Population Guangzhou: ['Guangzhou has a population of 15 million inhabitants as of 2021.']\n",
    "    Population Shanghai: '26 million (2019)'\n",
    "\n",
    "    {\"thought\": \"Now I know that Shanghai has the highest population.\", \"code\": \"final_answer(\\\"Shanghai\\\")\"}\n",
    "\n",
    "    ---\n",
    "    Task: \"What is the current age of the pope, raised to the power 0.36?\"\n",
    "\n",
    "    {\"thought\": \"I will use the tool `wikipedia_search` to get the age of the pope, and confirm that with a web search.\", \"code\": \"pope_age_wiki = wikipedia_search(query=\\\"current pope age\\\")\\nprint(\\\"Pope age as per wikipedia:\\\", pope_age_wiki)\\npope_age_search = web_search(query=\\\"current pope age\\\")\\nprint(\\\"Pope age as per google search:\\\", pope_age_search)\"}\n",
    "    Observation:\n",
    "    Pope age: \"The pope Francis is currently 88 years old.\"\n",
    "\n",
    "    {\"thought\": \"I know that the pope is 88 years old. Let's compute the result using python code.\", \"code\": \"pope_current_age = 88 ** 0.36\\nfinal_answer(pope_current_age)\"}\n",
    "\n",
    "    Above example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you only have access to these tools, behaving like regular python functions:\n",
    "    ```python\n",
    "    def final_answer(answer: any) -> any:\n",
    "        \"\"\"Provides a final answer to the given problem.\n",
    "\n",
    "        Args:\n",
    "            answer: The final answer to the problem\n",
    "        \"\"\"\n",
    "\n",
    "    ```\n",
    "\n",
    "    Here are the rules you should always follow to solve your task:\n",
    "    1. Use only variables that you have defined!\n",
    "    2. Always use the right arguments for the tools. DO NOT pass the arguments as a dict as in 'answer = wikipedia_search({'query': \"What is the place where James Bond lives?\"})', but use the arguments directly as in 'answer = wikipedia_search(query=\"What is the place where James Bond lives?\")'.\n",
    "    3. Take care to not chain too many sequential tool calls in the same code block, especially when the output format is unpredictable. For instance, a call to wikipedia_search has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block.\n",
    "    4. Call a tool only when needed, and never re-do a tool call that you previously did with the exact same parameters.\n",
    "    5. Don't name any new variable with the same name as a tool: for instance don't name a variable 'final_answer'.\n",
    "    6. Never create any notional variables in our code, as having these in your logs will derail you from the true variables.\n",
    "    7. You can use imports in your code, but only from the following list of modules: ['collections', 'datetime', 'itertools', 'math', 'queue', 'random', 're', 'stat', 'statistics', 'time', 'unicodedata']\n",
    "    8. The state persists between code executions: so if in one step you've created variables or imported modules, these will all persist.\n",
    "    9. Don't give up! You're in charge of solving the task, not providing directions to solve it.\n",
    "\n",
    "    Now Begin!\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af1a604",
   "metadata": {},
   "source": [
    "#### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305b671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper methods\n",
    "import os\n",
    "from smolagents import OpenAIServerModel, LiteLLMModel\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def set_agent_prompt(\n",
    "        agent, \n",
    "        type_of_prompt:str = \"system_prompt\", \n",
    "        prompt:str =    \n",
    "            \"\"\"Your a simple agent trying to solve problems using python code.\n",
    "            You have access to a simple python interpreter, and potentially tools,\n",
    "            when you have an answer use the final_answer tool \n",
    "            \"\"\"\n",
    "):\n",
    "    agent.prompt_templates[type_of_prompt] = prompt\n",
    "\n",
    "local_model_id = \"ollama_chat/qwen2.5-coder:7b-instruct-q8_0\"\n",
    "default_api_base = \"https://api.openai.com/v1\"\n",
    "default_model_id = \"gpt-4.1-nano\"\n",
    "    \n",
    "def load_model(model_id: str = default_model_id, api_base: str = default_api_base):\n",
    "\n",
    "    \n",
    "    api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not model_id:\n",
    "        return OpenAIServerModel(\n",
    "            model_id=model_id,\n",
    "            api_base=default_api_base,\n",
    "            api_key=api_key,\n",
    "\n",
    "        )\n",
    "    elif model_id.lower() == \"local\":\n",
    "        return LiteLLMModel(\n",
    "            model_id=local_model_id,\n",
    "            api_base=\"http://localhost:11434\",\n",
    "            num_ctx=16384,\n",
    "        )\n",
    "    else: \n",
    "        return OpenAIServerModel(\n",
    "            model_id=model_id,\n",
    "            api_base=api_base,\n",
    "            api_key=api_key,\n",
    "        )  \n",
    "    \n",
    "\n",
    "model = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025228dc",
   "metadata": {},
   "source": [
    "## Simple Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e20e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple processor\n",
    "from smolagents import CodeAgent\n",
    "code_agent = CodeAgent(\n",
    "    tools=[], \n",
    "    model=model,\n",
    "    name=\"simple_processor\",\n",
    "    description=\"Your a Simple processor, acting as a chatbot serving chat input, you should answer in plan text\",\n",
    "    verbosity_level=2, # only result, 1 = show steps and code executed, 2 = also shows output from LLM \n",
    "    stream_outputs=True, # Streams the thoughts of the agent\n",
    "    use_structured_outputs_internally=True, # Helps formatting the output in between steps improve performance for many models / Having a format allows for a better solution \n",
    "    # max_steps=3,\n",
    ")\n",
    "\n",
    "\n",
    "# set_agent_prompt(code_agent, prompt=better_prompt)\n",
    "query = input()\n",
    "\n",
    "code_agent.run(query)\n",
    "# code_agent.memory.steps.append(te)\n",
    "\n",
    "# code_agent.replay()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c82fad4",
   "metadata": {},
   "source": [
    "## Tool calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943bb2a5",
   "metadata": {},
   "source": [
    "### Simple tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5079900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool Calling (Augmented LLM & Tool utility)\n",
    "from smolagents import ToolCallingAgent, WebSearchTool, VisitWebpageTool, tool\n",
    "\n",
    "@tool\n",
    "def this_is_a_cool_tool(type_of_quote: str = \"\") -> str:\n",
    "    \"\"\"\n",
    "    Provides the caller with a cool or inspiring quote.\n",
    "\n",
    "    Args:\n",
    "        type_of_quote (str): The type of quote to provide. Examples include \"inspiring\", \"funny\", \"motivational\", etc.\n",
    "        \n",
    "    Returns:\n",
    "        A string containing the cool or inspiring quote.\n",
    "    \"\"\"\n",
    "\n",
    "    return \"This is an inspiring quote\"\n",
    "\n",
    "\n",
    "\n",
    "tool_agent = ToolCallingAgent(\n",
    "    tools=[WebSearchTool(), VisitWebpageTool(), this_is_a_cool_tool], \n",
    "    model=model,\n",
    "    name=\"tool_agent\",\n",
    "    description=\"Your a tool calling agent that runs web queries, can visit websites and provide inspiring quotes\",\n",
    "    # verbosity_level=2,\n",
    "    # stream_outputs=True,\n",
    "    max_steps=3\n",
    "\n",
    ")\n",
    "\n",
    "query = input()\n",
    "if not query.strip():\n",
    "    query = \"Give me an inspiring quote\"\n",
    " \n",
    "tool_agent.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ae19bf",
   "metadata": {},
   "source": [
    "### Advanced tooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca33f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q python-dotenv\n",
    "\n",
    "import json\n",
    "import os\n",
    "from smolagents import Tool\n",
    "from huggingface_hub import login\n",
    "token = os.environ[\"HF_WRITE_API_KEY\"]\n",
    "\n",
    "login(token)\n",
    "\n",
    "# download tools fra spaces\n",
    "\n",
    "image_generation_tool = Tool.from_space(\n",
    "    \"black-forest-labs/FLUX.1-schnell\",\n",
    "    name=\"image_generator\",\n",
    "    description=\"Generate an image from a prompt\"\n",
    ")\n",
    "tools = [WebSearchTool(), VisitWebpageTool(),image_generation_tool] \n",
    "\n",
    "# quotes_tool = load_tool(\"m-ric/Quotes\", trust_remote_code=True)\n",
    "\n",
    "tool_agent = ToolCallingAgent(\n",
    "    tools=tools, \n",
    "    model=model,\n",
    "    name=\"tool_agent\",\n",
    "    description=\"Your a tool calling agent that runs web queries, can visit websites and use other tools\",\n",
    "    max_steps=3\n",
    ")\n",
    "\n",
    "# query = input(\"what image would you like to generate\")\n",
    "# tool_agent.run(f\"can you generate me an image of a {query} and put it into the 'image' folder\")\n",
    "\n",
    "\n",
    "\n",
    "%pip install -q google-api-python-client\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "\n",
    "class YouTubeSearchTool(Tool):\n",
    "    name = \"youtube_search\"\n",
    "    description = (\n",
    "        \"Searches YouTube for videos matching a query. \"\n",
    "        \"Returns the top video titles and URLs.\"\n",
    "    )\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Search term for querying YouTube videos\",\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"  \n",
    "\n",
    "\n",
    "    def setup(self):\n",
    "        # Called once per tool instance, useful for setting up API key etc.\n",
    "        # You might load it from env var or config\n",
    "        load_dotenv()\n",
    "        self.api_key = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "        self.youtube = build(\"youtube\", \"v3\", developerKey=self.api_key)\n",
    "\n",
    "    def forward(self, query: str):\n",
    "        req = self.youtube.search().list(q=query, part=\"snippet\", type=\"video\", maxResults=5)\n",
    "        res = req.execute()\n",
    "        lst = [\n",
    "            {\n",
    "                \"title\": item[\"snippet\"][\"title\"],\n",
    "                \"url\": f\"https://www.youtube.com/watch?v={item['id']['videoId']}\",\n",
    "            }\n",
    "            for item in res[\"items\"]\n",
    "        ]\n",
    "        output = json.dumps(lst)\n",
    "        return output\n",
    "\n",
    "yt_search = YouTubeSearchTool()\n",
    "tool_agent.tools[yt_search.name] = yt_search \n",
    "\n",
    "\n",
    "query = input(\"What youtuber do you want to find\")\n",
    "if query:\n",
    "    tool_agent.run(f\"Find the latest video and also the most popular video from {query}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff4fa49",
   "metadata": {},
   "source": [
    "## Multi step agent - Human in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Step** (Reasoning in time or steps, No over path)\n",
    "from smolagents import CodeAgent, PlanningStep, DuckDuckGoSearchTool\n",
    "def interrupt_after_plan(memory_step, agent):\n",
    "    if isinstance(memory_step, PlanningStep):\n",
    "        print(\"Agent's plan: \" + str(agent.name))\n",
    "        print(memory_step.plan)\n",
    "        user_feedback = input(\"Do you want to modify the plan or continue? (Type 'modify' to edit, anything else to continue): \")\n",
    "        if user_feedback.strip().lower() == \"modify\":\n",
    "            new_plan = input(\"Enter your modified plan: \")\n",
    "            memory_step.plan = new_plan\n",
    "            print(\"Plan updated.\")\n",
    "            # Loop until user confirms the updated plan\n",
    "            while True:\n",
    "                print(\"Updated plan:\")\n",
    "                print(memory_step.plan)\n",
    "                confirm = input(\"Do you confirm this updated plan? (Type 'yes' to confirm, anything else to edit again): \")\n",
    "                if confirm.strip().lower() == \"yes\":\n",
    "                    break\n",
    "                new_plan = input(\"Enter your modified plan: \")\n",
    "                memory_step.plan = new_plan\n",
    "                print(\"Plan updated.\")\n",
    "        else:\n",
    "            print(\"Continuing with current plan.\")\n",
    "\n",
    "\n",
    "multi_step = CodeAgent(\n",
    "    model=model,\n",
    "    tools=[DuckDuckGoSearchTool()],\n",
    "    name=\"multi_step_agent\",\n",
    "    description=\"Multi step that solves task, you make a plan and also ask the human for feedback\",\n",
    "    planning_interval=3,  # Plan every 3 steps\n",
    "    step_callbacks={PlanningStep: interrupt_after_plan},  # Register callback for PlanningStep\n",
    "    max_steps=10,\n",
    ")\n",
    "task = input()\n",
    "\n",
    "multi_step.run(task, reset=False)  # Keeps all previous steps, NB: also from previous runs\n",
    "\n",
    "print(f\"Current memory contains {len(multi_step.memory.steps)} steps:\")\n",
    "for i, step in enumerate(multi_step.memory.steps):\n",
    "    step_type = type(step).__name__\n",
    "    print(f\"  {i+1}. {step_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adeb8ad8",
   "metadata": {},
   "source": [
    "## Multi-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98df9d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Agent (One agent, orchestrates ithers another agent)\n",
    "\n",
    "web_agent = ToolCallingAgent(\n",
    "    tools=[VisitWebpageTool(), WebSearchTool()],\n",
    "    model=model,\n",
    "    name=\"web_agent\",\n",
    "    description=\"Search the web and visit webpages\",\n",
    "    verbosity_level=0,\n",
    "    stream_outputs=True,\n",
    "    max_steps=10\n",
    ")\n",
    "\n",
    "code_agent = CodeAgent(\n",
    "    tools=[], \n",
    "    model=model,\n",
    "    name=\"code_agent\",\n",
    "    description=\"You are a managing agent which can solve problems but also use managede agents\",\n",
    "    # verbosity_level=0,\n",
    "    # stream_outputs=True,\n",
    "    use_structured_outputs_internally=True,\n",
    "    managed_agents=[web_agent],\n",
    "    max_steps=10\n",
    ")\n",
    "\n",
    "query = input()\n",
    "code_agent.run(query)\n",
    "\n",
    "replay = input(\"Would you like to replay the main agent's steps? (Press Enter to skip): \")\n",
    "if replay:\n",
    "    code_agent.replay()\n",
    "\n",
    "replay = input(\"Would you like to replay the web agent's steps? (Press Enter to skip): \")\n",
    "if replay:\n",
    "    code_agent.managed_agents[\"web_agent\"].replay()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aef393",
   "metadata": {},
   "source": [
    "## Augmented LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237da293",
   "metadata": {},
   "source": [
    "### Setup RAG tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d10b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q smolagents pandas langchain langchain-community sentence-transformers datasets python-dotenv rank_bm25 --upgrade\n",
    "\n",
    "import datasets\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# Load the Hugging Face documentation dataset\n",
    "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "\n",
    "# Filter to include only Transformers documentation\n",
    "knowledge_base = knowledge_base.filter(lambda row: row[\"source\"].startswith(\"huggingface/transformers\"))\n",
    "\n",
    "# Convert dataset entries to Document objects with metadata\n",
    "source_docs = [\n",
    "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
    "    for doc in knowledge_base\n",
    "]\n",
    "\n",
    "# Split documents into smaller chunks for better retrieval\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,  # Characters per chunk\n",
    "    chunk_overlap=50,  # Overlap between chunks to maintain context\n",
    "    add_start_index=True,\n",
    "    strip_whitespace=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],  # Priority order for splitting\n",
    ")\n",
    "docs_processed = text_splitter.split_documents(source_docs)\n",
    "\n",
    "print(f\"Knowledge base prepared with {len(docs_processed)} document chunks\")\n",
    "\n",
    "\n",
    "from smolagents import Tool\n",
    "\n",
    "class RetrieverTool(Tool):\n",
    "    name = \"retriever\"\n",
    "    description = \"Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query.\"\n",
    "    inputs = {\n",
    "        \"query\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
    "        }\n",
    "    }\n",
    "    output_type = \"string\"\n",
    "\n",
    "    def __init__(self, docs, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Initialize the retriever with our processed documents\n",
    "        self.retriever = BM25Retriever.from_documents(\n",
    "            docs, k=10  # Return top 10 most relevant documents\n",
    "        )\n",
    "\n",
    "    def forward(self, query: str) -> str:\n",
    "        \"\"\"Execute the retrieval based on the provided query.\"\"\"\n",
    "        assert isinstance(query, str), \"Your search query must be a string\"\n",
    "\n",
    "        # Retrieve relevant documents\n",
    "        docs = self.retriever.invoke(query)\n",
    "\n",
    "        # Format the retrieved documents for readability\n",
    "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
    "            [\n",
    "                f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n",
    "                for i, doc in enumerate(docs)\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b0627f",
   "metadata": {},
   "source": [
    "### Augmented LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09845a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our retriever tool with the processed documents\n",
    "retriever_tool = RetrieverTool(docs_processed)\n",
    "\n",
    "agent = CodeAgent(\n",
    "    tools=[retriever_tool],  \n",
    "    model=model,  \n",
    "    max_steps=4, \n",
    "    verbosity_level=2,  # Show detailed agent reasoning\n",
    ")\n",
    "\n",
    "# Ask a question that requires retrieving information\n",
    "question = \"For a transformers model training, which is slower, the forward or the backward pass?\"\n",
    "\n",
    "# Run the agent to get an answer\n",
    "agent_output = agent.run(question)\n",
    "\n",
    "# Display the final answer\n",
    "print(\"\\nFinal answer:\")\n",
    "print(agent_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac850e",
   "metadata": {},
   "source": [
    "## Fully Autonomous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Autonomous Agent** (No fixed path, no fixed Steps)\n",
    "%pip install -q google-search-results\n",
    "from smolagents import ToolCollection, tools\n",
    "from langchain.agents import load_tools\n",
    "\n",
    "# from langchain_community.agent_toolkits.load_tools import get_all_tool_names\n",
    "\n",
    "# tool_names = get_all_tool_names()\n",
    "# print(tool_names)\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "tool = load_tools([\"metaphor-search\"])[0]\n",
    "\n",
    "temp_tool = Tool.from_langchain(tool)\n",
    "\n",
    "model = load_model()\n",
    "tools = []\n",
    "tools.append(temp_tool)\n",
    "code_agent = CodeAgent(\n",
    "    model=model, \n",
    "    tools=tools, \n",
    "    use_structured_outputs_internally=True,\n",
    "    stream_outputs=True,\n",
    "    max_steps=10\n",
    ")\n",
    "# set_agent_prompt(code_agent)\n",
    "\n",
    "query = \"Give me a metaphor\"\n",
    "code_agent.run(query)\n",
    "\n",
    "# Access til nye tools\n",
    "\n",
    "search_tool = Tool.from_langchain(load_tools([\"serpapi\"])[0])\n",
    "\n",
    "code_agent.tools[search_tool.name] = search_tool\n",
    "print(search_tool.name)\n",
    "\n",
    "\n",
    "code_agent.run(\"Can you find the best restaraunts to bring a date in Copenhagen, it should be affordable, also help me plan activities for before and after\")\n",
    "\n",
    "problem = input()\n",
    "if problem:\n",
    "    query = f\"Find the best tool on langchain or hugginface to solve {problem}, output it such that i can be input directly into Tool.from_langchain(load_tools(['YouTubeSearch'])[0])\"\n",
    "    tool = code_agent.run(query)\n",
    "    code_agent.tools[tool.name] = tool\n",
    "\n",
    "\n",
    "use_tool = input()\n",
    "if use_tool:\n",
    "    code_agent.run(use_tool)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c2383",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - **Workflow** (Multi-step reasoning, outside of workflow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63924913",
   "metadata": {},
   "source": [
    "## Extra features\n",
    "\n",
    "An example from smolagents repo, trying to replicate the deep reasearch feature. This agent achieves 55% pass@1 on the GAIA validation set, compared to 67% for the original Deep Research.\n",
    "- https://github.com/huggingface/smolagents/blob/83ff2f7ed09788e130965349929f8bd5152e507e/examples/open_deep_research/run.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50c0f73",
   "metadata": {},
   "source": [
    "### Multi-step ui interactive\n",
    "\n",
    "Doesn't run in jupyter notebook, run gradiotest.py file insteads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac41f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"gradio\"\n",
    "%pip install -q 'smolagents[gradio]'\n",
    "%pip install --upgrade -q websockets\n",
    "%pip show gradio\n",
    "# %gradio  app.py hot reload \n",
    "# %gradio --vibe app.py, allow for text editor in the browser\n",
    "\n",
    "from smolagents import CodeAgent, GradioUI, load_tool\n",
    "image_generation_tool = load_tool(\"m-ric/text-to-image\", trust_remote_code=True)\n",
    "\n",
    "model = load_model()\n",
    "agent = CodeAgent(tools=[image_generation_tool], model=model)\n",
    "# gradio_ui = GradioUI(\n",
    "#     agent, \n",
    "#     file_upload_folder=\"./uploads\", \n",
    "#     reset_agent_memory=False # Set to true to agent memory inbetween interactions\n",
    "# ) \n",
    "# gradio_ui.launch(inline=True) # Inline should allow for running in a jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6904d17a",
   "metadata": {},
   "source": [
    "### Load and push agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4979ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "token = os.environ[\"HF_WRITE_API_KEY\"]\n",
    "\n",
    "login(token)\n",
    "\n",
    "# # Load agents \n",
    "\n",
    "# agent = CodeAgent.from_hub(\"AgenticSolutions/character_roleplay_chatbot\", trust_remote_code=True)\n",
    "\n",
    "\n",
    "# Upload agent \n",
    "code_agent.push_to_hub(\"Thomasitu/thesis\", private=True)\n",
    "# Repo: https://huggingface.co/spaces/Thomasitu/thesis/tree/main \n",
    "# Laver en Agent.JSON, gemmer configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00429c6",
   "metadata": {},
   "source": [
    "### Adjust agent memory, reuse, memory from other agents etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba89ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import InferenceClientModel, CodeAgent, ActionStep, TaskStep\n",
    "\n",
    "agent = CodeAgent(tools=[], model=InferenceClientModel(), verbosity_level=1)\n",
    "agent.python_executor.send_tools({**agent.tools})\n",
    "print(agent.memory.system_prompt)\n",
    "\n",
    "task = \"What is the 20th Fibonacci number?\"\n",
    "\n",
    "# You could modify the memory as needed here by inputting the memory of another agent.\n",
    "# agent.memory.steps = previous_agent.memory.steps\n",
    "\n",
    "# Let's start a new task!\n",
    "agent.memory.steps.append(TaskStep(task=task, task_images=[]))\n",
    "\n",
    "final_answer = None\n",
    "step_number = 1\n",
    "while final_answer is None and step_number <= 10:\n",
    "    memory_step = ActionStep(\n",
    "        step_number=step_number,\n",
    "        observations_images=[],\n",
    "    )\n",
    "    # Run one step.\n",
    "    final_answer = agent.step(memory_step)\n",
    "    agent.memory.steps.append(memory_step)\n",
    "    step_number += 1\n",
    "\n",
    "    # Change the memory as you please!\n",
    "    # For instance to update the latest step:\n",
    "    # agent.memory.steps[-1] = ...\n",
    "\n",
    "print(\"The final answer is:\", final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b22b27",
   "metadata": {},
   "source": [
    "## Secure environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4824f372",
   "metadata": {},
   "source": [
    "#### E2B Sandbox\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee710970",
   "metadata": {},
   "source": [
    "##### Single agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4db8909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"E2B_API_KEY\")\n",
    "\n",
    "model = load_model()\n",
    "with CodeAgent(model=model, tools=[], executor_type=\"e2b\",executor_kwargs=({\"api_key\": os.getenv(\"E2B_API_KEY\")})) as agent:\n",
    "    agent.run(\"Can you give me the 100th Fibonacci number?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55560b",
   "metadata": {},
   "source": [
    "##### Multi-agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from e2b_code_interpreter import Sandbox\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"E2B_API_KEY\")\n",
    "# Create the sandbox\n",
    "sandbox = Sandbox(api_key=api_key)\n",
    "\n",
    "# Install required packages\n",
    "sandbox.commands.run(\"pip install smolagents\")\n",
    "sandbox.commands.run(\"pip install openai\")\n",
    "\n",
    "def run_code_raise_errors(sandbox, code: str, verbose: bool = False) -> str:\n",
    "    execution = sandbox.run_code(\n",
    "        code,\n",
    "        envs={'OPENAI_API_KEY': os.getenv('OPENAI_API_KEY')}\n",
    "    )\n",
    "    if execution.error:\n",
    "        execution_logs = \"\\n\".join([str(log) for log in execution.logs.stdout])\n",
    "        logs = execution_logs\n",
    "        logs += execution.error.traceback\n",
    "        raise ValueError(logs)\n",
    "    return \"\\n\".join([str(log) for log in execution.logs.stdout])\n",
    "\n",
    "# Define your agent application\n",
    "agent_code = \"\"\"\n",
    "import os\n",
    "from smolagents import CodeAgent, OpenAIServerModel\n",
    "\n",
    "\n",
    "default_api_base = 'https://api.openai.com/v1'\n",
    "default_model_id = 'gpt-4.1-nano'\n",
    "token = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "model = OpenAIServerModel(\n",
    "            model_id=default_model_id,\n",
    "            api_base=default_api_base,\n",
    "            api_key=token\n",
    "        )\n",
    "\n",
    "\n",
    "# Initialize the agents\n",
    "agent = CodeAgent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    name=\"coder_agent\",\n",
    "    description=\"This agent takes care of your difficult algorithmic problems using code.\"\n",
    ")\n",
    "\n",
    "manager_agent = CodeAgent(\n",
    "    model=model,\n",
    "    tools=[],\n",
    "    managed_agents=[agent],\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "response = manager_agent.run(\"What's the 20th Fibonacci number?\")\n",
    "print(response)\n",
    "\"\"\"\n",
    "\n",
    "# Run the agent code in the sandbox\n",
    "execution_logs = run_code_raise_errors(sandbox, agent_code)\n",
    "print(execution_logs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4fbe9c",
   "metadata": {},
   "source": [
    "#### Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f9236",
   "metadata": {},
   "source": [
    "##### Single agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60168be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q 'smolagents[docker]'\n",
    "from smolagents import CodeAgent\n",
    "\n",
    "with CodeAgent(model=model, tools=[], executor_type=\"docker\") as agent:\n",
    "    agent.run(\"Can you give me the 100th Fibonacci number?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc43a30",
   "metadata": {},
   "source": [
    "##### Multi-agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4c4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docker\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class DockerSandbox:\n",
    "    def __init__(self):\n",
    "        self.client = docker.from_env()\n",
    "        self.container = None\n",
    "\n",
    "    def create_container(self):\n",
    "        try:\n",
    "            image, build_logs = self.client.images.build(\n",
    "                path=\".\",\n",
    "                tag=\"agent-sandbox\",\n",
    "                rm=True,\n",
    "                forcerm=True,\n",
    "                buildargs={},\n",
    "                # decode=True\n",
    "            )\n",
    "        except docker.errors.BuildError as e:\n",
    "            print(\"Build error logs:\")\n",
    "            for log in e.build_log:\n",
    "                if 'stream' in log:\n",
    "                    print(log['stream'].strip())\n",
    "            raise\n",
    "\n",
    "        # Create container with security constraints and proper logging\n",
    "        self.container = self.client.containers.run(\n",
    "            \"agent-sandbox\",\n",
    "            command=\"tail -f /dev/null\",  # Keep container running\n",
    "            detach=True,\n",
    "            tty=True,\n",
    "            mem_limit=\"512m\",\n",
    "            cpu_quota=50000,\n",
    "            pids_limit=100,\n",
    "            security_opt=[\"no-new-privileges\"],\n",
    "            cap_drop=[\"ALL\"],\n",
    "            environment={\n",
    "                \"OPENAI_API_KEY\": os.getenv(\"OPENAI_API_KEY\")\n",
    "            },\n",
    "        )\n",
    "\n",
    "    def run_code(self, code: str) -> Optional[str]:\n",
    "        if not self.container:\n",
    "            self.create_container()\n",
    "\n",
    "        # Execute code in container\n",
    "        exec_result = self.container.exec_run(\n",
    "            cmd=[\"python\", \"-c\", code],\n",
    "            user=\"nobody\"\n",
    "        )\n",
    "\n",
    "        # Collect all output\n",
    "        return exec_result.output.decode() if exec_result.output else None\n",
    "\n",
    "\n",
    "    def cleanup(self):\n",
    "        if self.container:\n",
    "            try:\n",
    "                self.container.stop()\n",
    "            except docker.errors.NotFound:\n",
    "                # Container already removed, this is expected\n",
    "                pass\n",
    "            except Exception as e:\n",
    "                print(f\"Error during cleanup: {e}\")\n",
    "            finally:\n",
    "                self.container = None  # Clear the reference\n",
    "\n",
    "# Example usage:\n",
    "sandbox = DockerSandbox()\n",
    "\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# Create the sandbox\n",
    "\n",
    "# Install required packages\n",
    "sandbox.run_code(\"pip install smolagents\")\n",
    "sandbox.run_code(\"pip install smolagents[openai]\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # Define your agent code\n",
    "    agent_code = \"\"\"\n",
    "\n",
    "import os\n",
    "from smolagents import CodeAgent, OpenAIServerModel \n",
    "\n",
    "default_api_base = 'https://api.openai.com/v1'\n",
    "default_model_id = 'gpt-4.1-nano'\n",
    "token = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "model = OpenAIServerModel(\n",
    "            model_id=default_model_id,\n",
    "            api_base=default_api_base,\n",
    "            api_key=token\n",
    "        )\n",
    "# Initialize the agent\n",
    "agent = CodeAgent(\n",
    "    model=model,\n",
    "    tools=[]\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "response = agent.run(\"What's the 20th Fibonacci number?\")\n",
    "print(response)\n",
    "\"\"\"\n",
    "\n",
    "    # Run the code in the sandbox\n",
    "    output = sandbox.run_code(agent_code)\n",
    "    print(output)\n",
    "\n",
    "finally:\n",
    "    sandbox.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122679c",
   "metadata": {},
   "source": [
    "## MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0ddb6f",
   "metadata": {},
   "source": [
    "### Setup local MCP Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955da02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FastMCP Desktop Example\n",
    "\n",
    "A simple example that exposes the desktop directory as a resource.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "# Create server\n",
    "mcp = FastMCP(\"Demo\")\n",
    "\n",
    "\n",
    "@mcp.resource(\"dir://desktop\")\n",
    "def desktop() -> list[str]:\n",
    "    \"\"\"List the files in the user's desktop\"\"\"\n",
    "    desktop = Path.home() / \"Desktop\"\n",
    "    return [str(f) for f in desktop.iterdir()]\n",
    "\n",
    "\n",
    "@mcp.tool()\n",
    "def sum(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return a + b\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bef44",
   "metadata": {},
   "source": [
    "### Handle lifecycle of MCP client automaticly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74ff40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q mcp\n",
    "%pip install -q 'smolagents[mcp]'\n",
    "from smolagents import MCPClient, CodeAgent\n",
    "import os\n",
    "from mcp import StdioServerParameters\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "server_parameters = StdioServerParameters(\n",
    "    command=\"uvx\",  # Using uvx ensures dependencies are available\n",
    "    args=[\"--quiet\", \"pubmedmcp@0.1.3\"],\n",
    "    env={\"UV_PYTHON\": \"3.12\", **os.environ},\n",
    ")\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "with MCPClient(server_parameters) as tools:\n",
    "    agent = CodeAgent(tools=tools, model=model, use_structured_outputs_internally=True, max_steps=10)\n",
    "    \n",
    "    query = input(\"Enter your query: \")\n",
    "    if query:\n",
    "        agent.run(\"Please find the latest research on COVID-19 treatment.\")\n",
    "\n",
    "    replay = input(\"Do you want to replay the steps? (Y/'')\")\n",
    "    if replay:\n",
    "        agent.replay()\n",
    "\n",
    "# local server:\n",
    "with MCPClient({\"url\": \"http://localhost:8000/sse\", \"transport\": \"sse\"}) as tools:\n",
    "    code_agent = CodeAgent(\n",
    "        model=model,\n",
    "        tools=tools, \n",
    "        name=\"code_agent\",\n",
    "        description=\"\",\n",
    "        max_steps=10,\n",
    "        use_structured_outputs_internally=True\n",
    "    )\n",
    "    \n",
    "    query = input(\"Enter your query: \")\n",
    "    code_agent.run(query) if query else None\n",
    "\n",
    "    replay = input(\"Do you want to replay the steps? (Y/'')\")\n",
    "    if replay:\n",
    "        agent.replay()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e5a6f3",
   "metadata": {},
   "source": [
    "### Manual lifecycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193b097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import StdioServerParameters\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "try:\n",
    "    mcp_client = MCPClient({\"url\": \"http://localhost:8000/sse\", \"transport\": \"sse\"})\n",
    "    tools = mcp_client.get_tools()\n",
    "\n",
    "    code_agent = CodeAgent()\n",
    "    # use your tools here.\n",
    "finally:\n",
    "    mcp_client.disconnect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2886c0d",
   "metadata": {},
   "source": [
    "### Use Multiple MCP Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0c10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize server parameters\n",
    "pubmed_server = StdioServerParameters(\n",
    "    command=\"uvx\",\n",
    "    args=[\"--quiet\", \"pubmedmcp@0.1.3\"],\n",
    "    env={\"UV_PYTHON\": \"3.12\", **os.environ},\n",
    ")\n",
    "\n",
    "local_server = {\"url\": \"http://127.0.0.1:8000/sse\"}\n",
    "\n",
    "\n",
    "# Manually manage the connection\n",
    "try:\n",
    "    mcp_client = MCPClient([pubmed_server, local_server])\n",
    "    tools = mcp_client.get_tools()\n",
    "\n",
    "    # Use the tools with your agent\n",
    "    agent = CodeAgent(tools=tools, model=model)\n",
    "    result = agent.run(\"What are the recent therapeutic approaches for Alzheimer's disease?\")\n",
    "\n",
    "    # Process the result as needed\n",
    "    print(f\"Agent response: {result}\")\n",
    "finally:\n",
    "    # Always ensure the connection is properly closed\n",
    "    mcp_client.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
